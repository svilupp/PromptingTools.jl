# Frequently Asked Questions

## Why OpenAI

OpenAI's models are at the forefront of AI research and provide robust, state-of-the-art capabilities for many tasks.

There will be situations not or cannot use it (eg, privacy, cost, etc.). In that case, you can use local models (eg, Ollama) or other APIs (eg, Anthropic).

Note: To get started with [Ollama.ai](https://ollama.ai/), see the [Setup Guide for Ollama](#setup-guide-for-ollama) section below.

## Data Privacy and OpenAI

At the time of writing, OpenAI does NOT use the API calls for training their models.

> **API**
> 
> OpenAI does not use data submitted to and generated by our API to train OpenAI models or improve OpenAI’s service offering. In order to support the continuous improvement of our models, you can fill out this form to opt-in to share your data with us. -- [How your data is used to improve our models](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)

You can always double-check the latest information on the [OpenAI's How we use your data](https://platform.openai.com/docs/models/how-we-use-your-data) page.

Resources:
- [OpenAI's How we use your data](https://platform.openai.com/docs/models/how-we-use-your-data)
- [Data usage for consumer services FAQ](https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq)
- [How your data is used to improve our models](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)


## Creating OpenAI API Key

You can get your API key from OpenAI by signing up for an account and accessing the API section of the OpenAI website.

1. Create an account with [OpenAI](https://platform.openai.com/signup)
2. Go to [API Key page](https://platform.openai.com/account/api-keys)
3. Click on “Create new secret key”
  !!! Do not share it with anyone and do NOT save it to any files that get synced online.

Resources:
- [OpenAI Documentation](https://platform.openai.com/docs/quickstart?context=python)
- [Visual tutorial](https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt)

Pro tip: Always set the spending limits!

## Setting OpenAI Spending Limits

OpenAI allows you to set spending limits directly on your account dashboard to prevent unexpected costs.

1. Go to [OpenAI Billing](https://platform.openai.com/account/billing)
2. Set Soft Limit (you’ll receive a notification) and Hard Limit (API will stop working not to spend more money)
 
A good start might be a soft limit of c.$5 and a hard limit of c.$10 - you can always increase it later in the month.

Resources:
- [OpenAI Forum](https://community.openai.com/t/how-to-set-a-price-limit/13086)

### How much does it cost? Is it worth paying for?

If you use a local model (eg, with Ollama), it's free. If you use any commercial APIs (eg, OpenAI), you will likely pay per "token" (a sub-word unit).

For example, a simple request with a simple question and 1 sentence response in return (”Is statement XYZ a positive comment”) will cost you ~$0.0001 (ie, one-hundredth of a cent)

**Is it worth paying for?**

GenAI is a way to buy time! You can pay cents to save tens of minutes every day.

Continuing the example above, imagine you have a table with 200 comments. Now, you can parse each one of them with an LLM for the features/checks you need. 
Assuming the price per call was $0.0001, you'd pay 2 cents for the job and save 30-60 minutes of your time!


Resources:
- [OpenAI Pricing per 1000 tokens](https://openai.com/pricing)

## Configuring the Environment Variable for API Key

This is a guide for OpenAI's API key, but it works for any other API key you might need (eg, `MISTRALAI_API_KEY` for MistralAI API).

To use the OpenAI API with PromptingTools.jl, set your API key as an environment variable:

```julia
ENV["OPENAI_API_KEY"] = "your-api-key"
```

As a one-off, you can: 
- set it in the terminal before launching Julia: `export OPENAI_API_KEY = <your key>`
- set it in your `setup.jl` (make sure not to commit it to GitHub!)

Make sure to start Julia from the same terminal window where you set the variable.
Easy check in Julia, run `ENV["OPENAI_API_KEY"]` and you should see your key!

A better way:
- On a Mac, add the configuration line to your terminal's configuration file (eg, `~/.zshrc`). It will get automatically loaded every time you launch the terminal
- On Windows, set it as a system variable in "Environment Variables" settings (see the Resources)

Resources: 
- [OpenAI Guide](https://platform.openai.com/docs/quickstart?context=python)

## Setting the API Key via Preferences.jl

You can also set the API key in `LocalPreferences.toml`, so it persists across sessions and projects.

Use: `PromptingTools.set_preferences!("OPENAI_API_KEY"="your-api-key")`

To double-check, run `PromptingTools.get_preferences("OPENAI_API_KEY")` and you should see your key!

See more detail in the `?PromptingTools.PREFERENCES` docstring.

## Understanding the API Keyword Arguments in `aigenerate` (`api_kwargs`)
  
See [OpenAI API reference](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) for more information.

## Instant Access from Anywhere

For easy access from anywhere, add PromptingTools into your `startup.jl` (can be found in `~/.julia/config/startup.jl`).

Add the following snippet:
```
using PromptingTools
const PT = PromptingTools # to access unexported functions and types
```

Now, you can just use `ai"Help me do X to achieve Y"` from any REPL session!

## Open Source Alternatives

The ethos of PromptingTools.jl is to allow you to use whatever model you want, which includes Open Source LLMs. The most popular and easiest to setup is [Ollama.ai](https://ollama.ai/) - see below for more information.

## Setup Guide for Ollama

Ollama runs a background service hosting LLMs that you can access via a simple API. It's especially useful when you're working with some sensitive data that should not be sent anywhere.

Installation is very easy, just download the latest version [here](https://ollama.ai/download).

Once you've installed it, just launch the app and you're ready to go!

To check if it's running, go to your browser and open `127.0.0.1:11434`. You should see the message "Ollama is running". 
Alternatively, you can run `ollama serve` in your terminal and you'll get a message that it's already running.

There are many models available in [Ollama Library](https://ollama.ai/library), including Llama2, CodeLlama, SQLCoder, or my personal favorite `openhermes2.5-mistral`.

Download new models with `ollama pull <model_name>` (eg, `ollama pull openhermes2.5-mistral`). 

Show currently available models with `ollama list`.

See [Ollama.ai](https://ollama.ai/) for more information.

## Changing the Default Model or Schema

If you tend to use non-default options, it can get tedious to specify `PT.*` every time.

There are three ways how you can customize your workflows (especially when you use Ollama or other local models):

1) Import the functions/types you need explicitly at the top (eg, `using PromptingTools: OllamaSchema`)
2) Register your model and its associated schema  (`PT.register_model!(; name="123", schema=PT.OllamaSchema())`). You won't have to specify the schema anymore only the model name. See [Working with Ollama](#working-with-ollama) for more information.
3) Override your default model (`PT.MODEL_CHAT`) and schema (`PT.PROMPT_SCHEMA`). It can be done persistently with Preferences, eg, `PT.set_preferences!("PROMPT_SCHEMA" => "OllamaSchema", "MODEL_CHAT"=>"llama2")`.