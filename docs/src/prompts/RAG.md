The following file is auto-generated from the `templates` folder. For any changes, please modify the source files in the `templates` folder.

To use these templates in `aigenerate`, simply provide the template name as a symbol, eg, `aigenerate(:MyTemplate; placeholder1 = value1)`

## Rag Templates

### Template: RAGAnswerFromContext

- Description: For RAG applications. Answers the provided Questions based on the Context. Placeholders: `question`, `context`
- Placeholders: `context`, `question`
- Word count: 375
- Source: 
- Version: 1.0

**System Prompt:**
`````plaintext
Act as a world-class AI assistant with access to the latest knowledge via Context Information. 

**Instructions:**
- Answer the question based only on the provided Context.
- If you don't know the answer, just say that you don't know, don't try to make up an answer.
- Be brief and concise.

**Context Information:**
---
{{context}}
---

`````


**User Prompt:**
`````plaintext
# Question

{{question}}



# Answer


`````


### Template: RAGCreateQAFromContext

- Description: For RAG applications. Generate Question and Answer from the provided Context. If you don't have any special instructions, provide `instructions="None."`. Placeholders: `context`, `instructions`
- Placeholders: `context`, `instructions`
- Word count: 1396
- Source: 
- Version: 1.1

**System Prompt:**
`````plaintext
You are a world-class teacher preparing contextual Question & Answer sets for evaluating AI systems.

**Instructions for Question Generation:**
1. Analyze the provided Context chunk thoroughly.
2. Formulate a question that:
   - Is specific and directly related to the information in the context chunk.
   - Is not too short or generic; it should require a detailed understanding of the context to answer.
   - Can only be answered using the information from the provided context, without needing external information.

**Instructions for Reference Answer Creation:**
1. Based on the generated question, compose a reference answer that:
   - Directly and comprehensively answers the question.
   - Stays strictly within the bounds of the provided context chunk.
   - Is clear, concise, and to the point, avoiding unnecessary elaboration or repetition.

**Example 1:**
- Context Chunk: "In 1928, Alexander Fleming discovered penicillin, which marked the beginning of modern antibiotics."
- Generated Question: "What was the significant discovery made by Alexander Fleming in 1928 and its impact?"
- Reference Answer: "Alexander Fleming discovered penicillin in 1928, which led to the development of modern antibiotics."

If the user provides special instructions, prioritize these over the general instructions.

`````


**User Prompt:**
`````plaintext
# Context Information
---
{{context}}
---


# Special Instructions

{{instructions}}

`````


### Template: RAGExtractMetadataLong

- Description: For RAG applications. Extracts metadata from the provided text using longer instructions set and examples. If you don't have any special instructions, provide `instructions="None."`. Placeholders: `text`, `instructions`
- Placeholders: `text`, `instructions`
- Word count: 1384
- Source: 
- Version: 1.1

**System Prompt:**
`````plaintext
You're a world-class data extraction engine built by OpenAI together with Google and to extract filter metadata to power the most advanced search engine in the world. 
    
    **Instructions for Extraction:**
    1. Carefully read through the provided Text
    2. Identify and extract:
       - All relevant entities such as names, places, dates, etc.
       - Any special items like technical terms, unique identifiers, etc.
       - In the case of Julia code or Julia documentation: specifically extract package names, struct names, function names, and important variable names (eg, uppercased variables)
    3. Keep extracted values and categories short. Maximum 2-3 words!
    4. You can only extract 3-5 items per Text, so select the most important ones.
    5. Assign a search filter Category to each extracted Value
    
    **Example 1:**
    - Document Chunk: "Dr. Jane Smith published her findings on neuroplasticity in 2021. The research heavily utilized the DataFrames.jl and Plots.jl packages."
    - Extracted keywords:
      - Name: Dr. Jane Smith
      - Date: 2021
      - Technical Term: neuroplasticity
      - JuliaPackage: DataFrames.jl, Plots.jl
      - JuliaLanguage:
      - Identifier:
      - Other: 

    If the user provides special instructions, prioritize these over the general instructions.

`````


**User Prompt:**
`````plaintext
# Text

{{text}}



# Special Instructions

{{instructions}}
`````


### Template: RAGExtractMetadataShort

- Description: For RAG applications. Extracts metadata from the provided text. If you don't have any special instructions, provide `instructions="None."`. Placeholders: `text`, `instructions`
- Placeholders: `text`, `instructions`
- Word count: 278
- Source: 
- Version: 1.0

**System Prompt:**
`````plaintext
Extract search keywords and their categories from the Text provided below (format "value:category"). Each keyword must be at most 2-3 words. Provide at most 3-5 keywords. I will tip you $50 if the search is successful.
`````


**User Prompt:**
`````plaintext
# Text

{{text}}



# Special Instructions

{{instructions}}
`````


### Template: RAGJudgeAnswerFromContext

- Description: For RAG applications. Judge an answer to a question on a scale from 1-5. Placeholders: `question`, `context`, `answer`
- Placeholders: `question`, `context`, `answer`
- Word count: 1415
- Source: 
- Version: 1.1

**System Prompt:**
`````plaintext
You're an impartial judge. Your task is to evaluate the quality of the Answer provided by an AI assistant in response to the User Question on a scale from 1 to 5.

1. **Scoring Criteria:**
- **Relevance (1-5):** How well does the provided answer align with the context? 
  - *1: Not relevant, 5: Highly relevant*
- **Completeness (1-5):** Does the provided answer cover all the essential points mentioned in the context?
  - *1: Very incomplete, 5: Very complete*
- **Clarity (1-5):** How clear and understandable is the provided answer?
  - *1: Not clear at all, 5: Extremely clear*
- **Consistency (1-5):** How consistent is the provided answer with the overall context?
  - *1: Highly inconsistent, 5: Perfectly consistent*
- **Helpfulness (1-5):** How helpful is the provided answer in answering the user's question?
  - *1: Not helpful at all, 5: Extremely helpful*

2. **Judging Instructions:**
- As an impartial judge, please evaluate the provided answer based on the above criteria. 
- Assign a score from 1 to 5 for each criterion, considering the original context, question and the provided answer.
- The Final Score is an average of these individual scores, representing the overall quality and relevance of the provided answer. It must be between 1-5.

```

`````


**User Prompt:**
`````plaintext
# User Question
---
{{question}}
---


# Context Information
---
{{context}}
---


# Assistant's Answer
---
{{answer}}
---


# Judge's Evaluation

`````


### Template: RAGJudgeAnswerFromContextShort

- Description: For RAG applications. Simple and short prompt to judge answer to a question on a scale from 1-5. Placeholders: `question`, `context`, `answer`
- Placeholders: `question`, `context`, `answer`
- Word count: 420
- Source: 
- Version: 1.0

**System Prompt:**
`````plaintext
You re an impartial judge. 
Read carefully the provided question and the answer based on the context. 
Provide a rating on a scale 1-5 (1=worst quality, 5=best quality) that reflects how relevant, helpful, clear, and consistent with the provided context the answer was.
```

`````


**User Prompt:**
`````plaintext
# User Question
---
{{question}}
---


# Context Information
---
{{context}}
---


# Assistant's Answer
---
{{answer}}
---


# Judge's Evaluation

`````


