import { _ as _export_sfc, c as createElementBlock, m as createBaseVNode, a as createTextVNode, t as toDisplayString, a7 as createStaticVNode, o as openBlock } from "./chunks/framework.CTE10pW1.js";
const __pageData = JSON.parse('{"title":"How It Works","description":"","frontmatter":{},"headers":[],"relativePath":"how_it_works.md","filePath":"how_it_works.md","lastUpdated":null}');
const _sfc_main = { name: "how_it_works.md" };
const _hoisted_1 = /* @__PURE__ */ createStaticVNode('<h1 id="How-It-Works" tabindex="-1">How It Works <a class="header-anchor" href="#How-It-Works" aria-label="Permalink to &quot;How It Works {#How-It-Works}&quot;">​</a></h1><p>This is an advanced section that explains how PromptingTools.jl works under the hood. It is not necessary to understand this to use the package, but it can be helpful for debugging and understanding the limitations of the package.</p><p>We&#39;ll start with the key concepts and then walk through an example of <code>aigenerate</code> to see how it all fits together.</p><h2 id="Key-Concepts" tabindex="-1">Key Concepts <a class="header-anchor" href="#Key-Concepts" aria-label="Permalink to &quot;Key Concepts {#Key-Concepts}&quot;">​</a></h2><p>5 Key Concepts (/Objects):</p><ul><li><p><strong>API/Model Providers</strong> -&gt; The method that gives you access to Large Language Models (LLM), it can be an API (eg, OpenAI) or a locally-hosted application (eg, Llama.cpp or Ollama)</p></li><li><p><strong>Schemas</strong> -&gt; object of type <code>AbstractPromptSchema</code> that determines which methods are called and, hence, what providers/APIs are used</p></li><li><p><strong>Prompts</strong> -&gt; the information you want to convey to the AI model</p></li><li><p><strong>Messages</strong> -&gt; the basic unit of communication between the user and the AI model (eg, <code>UserMessage</code> vs <code>AIMessage</code>)</p></li><li><p><strong>Prompt Templates</strong> -&gt; re-usable &quot;prompts&quot; with placeholders that you can replace with your inputs at the time of making the request</p></li></ul><p>When you call <code>aigenerate</code>, roughly the following happens: <code>render</code> -&gt; <code>UserMessage</code>(s) -&gt; <code>render</code> -&gt; <code>OpenAI.create_chat</code> -&gt; ... -&gt; <code>AIMessage</code>.</p><h3 id="API/Model-Providers" tabindex="-1">API/Model Providers <a class="header-anchor" href="#API/Model-Providers" aria-label="Permalink to &quot;API/Model Providers {#API/Model-Providers}&quot;">​</a></h3><p>You can think of &quot;API/Model Providers&quot; as the method that gives you access to Large Language Models (LLM). It can be an API (eg, OpenAI) or a locally-hosted application (eg, Llama.cpp or Ollama).</p><p>You interact with them via the <code>schema</code> object, which is a subtype of <code>AbstractPromptSchema</code>, eg, there is an <code>OpenAISchema</code> for the provider &quot;OpenAI&quot; and its supertype <code>AbstractOpenAISchema</code> is for all other providers that mimic the OpenAI API.</p><h3 id="Schemas" tabindex="-1">Schemas <a class="header-anchor" href="#Schemas" aria-label="Permalink to &quot;Schemas {#Schemas}&quot;">​</a></h3><p>For your &quot;message&quot; to reach an AI model, it needs to be formatted and sent to the right place (-&gt; provider!).</p><p>We leverage the multiple dispatch around the &quot;schemas&quot; to pick the right logic. All schemas are subtypes of <code>AbstractPromptSchema</code> and there are many subtypes, eg, <code>OpenAISchema &lt;: AbstractOpenAISchema &lt;:AbstractPromptSchema</code>.</p><p>For example, if you provide <code>schema = OpenAISchema()</code>, the system knows that:</p><ul><li><p>it will have to format any user inputs to OpenAI&#39;s &quot;message specification&quot; (a vector of dictionaries, see their API documentation). Function <code>render(OpenAISchema(),...)</code> will take care of the rendering.</p></li><li><p>it will have to send the message to OpenAI&#39;s API. We will use the amazing <code>OpenAI.jl</code> package to handle the communication.</p></li></ul><h3 id="Prompts" tabindex="-1">Prompts <a class="header-anchor" href="#Prompts" aria-label="Permalink to &quot;Prompts {#Prompts}&quot;">​</a></h3><p>Prompt is loosely the information you want to convey to the AI model. It can be a question, a statement, or a command. It can have instructions or some context, eg, previous conversation.</p><p>You need to remember that Large Language Models (LLMs) are <strong>stateless</strong>. They don&#39;t remember the previous conversation/request, so you need to provide the whole history/context every time (similar to how REST APIs work).</p><p>Prompts that we send to the LLMs are effectively a sequence of messages (<code>&lt;:AbstractMessage</code>).</p><h3 id="Messages" tabindex="-1">Messages <a class="header-anchor" href="#Messages" aria-label="Permalink to &quot;Messages {#Messages}&quot;">​</a></h3><p>Messages are the basic unit of communication between the user and the AI model.</p><p>There are 5 main types of messages (<code>&lt;:AbstractMessage</code>):</p><ul><li><p><code>SystemMessage</code> - this contains information about the &quot;system&quot;, eg, how it should behave, format its output, etc. (eg, `You&#39;re a world-class Julia programmer. You write brief and concise code.)</p></li><li><p><code>UserMessage</code> - the information &quot;from the user&quot;, ie, your question/statement/task</p></li><li><p><code>UserMessageWithImages</code> - the same as <code>UserMessage</code>, but with images (URLs or Base64-encoded images)</p></li><li><p><code>AIMessage</code> - the response from the AI model, when the &quot;output&quot; is text</p></li><li><p><code>DataMessage</code> - the response from the AI model, when the &quot;output&quot; is data, eg, embeddings with <code>aiembed</code> or user-defined structs with <code>aiextract</code></p></li></ul><h3 id="Prompt-Templates" tabindex="-1">Prompt Templates <a class="header-anchor" href="#Prompt-Templates" aria-label="Permalink to &quot;Prompt Templates {#Prompt-Templates}&quot;">​</a></h3>', 24);
const _hoisted_25 = /* @__PURE__ */ createStaticVNode('<p>&quot;AI Templates&quot; as we call them (<code>AITemplate</code>) are usually a vector of <code>SystemMessage</code> and a <code>UserMessage</code> with specific purpose/task.</p><p>For example, the template <code>:AssistantAsk</code> is defined loosely as:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> template </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">SystemMessage</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You are a world-class AI assistant. Your communication is brief and concise. You&#39;re precise and answer only when you&#39;re confident in the high quality of your answer.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">             UserMessage</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;# Question</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{{ask}}&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)]</span></span></code></pre></div>', 3);
const _hoisted_28 = /* @__PURE__ */ createBaseVNode("code", null, "ask", -1);
const _hoisted_29 = /* @__PURE__ */ createStaticVNode('<p>When you provide a Symbol (eg, <code>:AssistantAsk</code>) to ai* functions, thanks to the multiple dispatch, it recognizes that it&#39;s an <code>AITemplate(:AssistantAsk)</code> and looks it up.</p><p>You can discover all available templates with <code>aitemplates(&quot;some keyword&quot;)</code> or just see the details of some template <code>aitemplates(:AssistantAsk)</code>.</p><p>Note: There is a new way to create and register templates in one go with <code>create_template(;user=&lt;user prompt&gt;, system=&lt;system prompt&gt;, load_as=&lt;template name&gt;)</code> (it skips the serialization step where a template previously must have been saved somewhere on the disk). See FAQ for more details or directly <code>?create_template</code>.</p><h3 id="ai*-Functions-Overview" tabindex="-1">ai* Functions Overview <a class="header-anchor" href="#ai*-Functions-Overview" aria-label="Permalink to &quot;ai* Functions Overview {#ai*-Functions-Overview}&quot;">​</a></h3><p>The above steps are implemented in the <code>ai*</code> functions, eg, <code>aigenerate</code>, <code>aiembed</code>, <code>aiextract</code>, etc. They all have the same basic structure:</p><p><code>ai*(&lt;optional schema&gt;,&lt;prompt or conversation&gt;; &lt;optional keyword arguments&gt;)</code>,</p><p>but they differ in purpose:</p><ul><li><p><code>aigenerate</code> is the general-purpose function to generate any text response with LLMs, ie, it returns <code>AIMessage</code> with field <code>:content</code> containing the generated text (eg, <code>ans.content isa AbstractString</code>)</p></li><li><p><code>aiembed</code> is designed to extract embeddings from the AI model&#39;s response, ie, it returns <code>DataMessage</code> with field <code>:content</code> containing the embeddings (eg, <code>ans.content isa AbstractArray</code>)</p></li><li><p><code>aiextract</code> is designed to extract structured data from the AI model&#39;s response and return them as a Julia struct (eg, if we provide <code>return_type=Food</code>, we get <code>ans.content isa Food</code>). You need to define the return type first and then provide it as a keyword argument.</p></li><li><p><code>aiclassify</code> is designed to classify the input text into (or simply respond within) a set of discrete <code>choices</code> provided by the user. It can be very useful as an LLM Judge or a router for RAG systems, as it uses the &quot;logit bias trick&quot; and generates exactly 1 token. It returns <code>AIMessage</code> with field <code>:content</code>, but the <code>:content</code> can be only one of the provided <code>choices</code> (eg, <code>ans.content in choices</code>)</p></li><li><p><code>aiscan</code> is for working with images and vision-enabled models (as an input), but it returns <code>AIMessage</code> with field <code>:content</code> containing the generated text (eg, <code>ans.content isa AbstractString</code>) similar to <code>aigenerate</code>.</p></li><li><p><code>aiimage</code> is for generating images (eg, with OpenAI DALL-E 3). It returns a <code>DataMessage</code>, where the field <code>:content</code> might contain either the URL to download the image from or the Base64-encoded image depending on the user-provided kwarg <code>api_kwargs.response_format</code>.</p></li><li><p><code>aitemplates</code> is a helper function to discover available templates and see their details (eg, <code>aitemplates(&quot;some keyword&quot;)</code> or <code>aitemplates(:AssistantAsk)</code>)</p></li></ul><p>If you&#39;re using a known <code>model</code>, you do NOT need to provide a <code>schema</code> (the first argument).</p><p>Optional keyword arguments in <code>ai*</code> tend to be:</p><ul><li><p><code>model::String</code> - Which model you want to use</p></li><li><p><code>verbose::Bool</code> - Whether you went to see INFO logs around AI costs</p></li><li><p><code>return_all::Bool</code> - Whether you want the WHOLE conversation or just the AI answer (ie, whether you want to include your inputs/prompt in the output)</p></li><li><p><code>api_kwargs::NamedTuple</code> - Specific parameters for the model, eg, <code>temperature=0.0</code> to be NOT creative (and have more similar output in each run)</p></li><li><p><code>http_kwargs::NamedTuple</code> - Parameters for the HTTP.jl package, eg, <code>readtimeout = 120</code> to time out in 120 seconds if no response was received.</p></li></ul><p>In addition to the above list of <code>ai*</code> functions, you can also use the <strong>&quot;lazy&quot; counterparts</strong> of these functions from the experimental AgentTools module.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Experimental</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">AgentTools</span></span></code></pre></div><p>For example, <code>AIGenerate()</code> will create a lazy instance of <code>aigenerate</code>. It is an instance of <code>AICall</code> with <code>aigenerate</code> as its ai function. It uses exactly the same arguments and keyword arguments as <code>aigenerate</code> (see <code>?aigenerate</code> for details).</p><p>&quot;lazy&quot; refers to the fact that it does NOT generate any output when instantiated (only when <code>run!</code> is called).</p><p>Or said differently, the <code>AICall</code> struct and all its flavors (<code>AIGenerate</code>, ...) are designed to facilitate a deferred execution model (lazy evaluation) for AI functions that interact with a Language Learning Model (LLM). It stores the necessary information for an AI call and executes the underlying AI function only when supplied with a <code>UserMessage</code> or when the <code>run!</code> method is applied.</p><p>This approach allows us to remember user inputs and trigger the LLM call repeatedly if needed, which enables automatic fixing (see <code>?airetry!</code>).</p><p>Example:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AIGenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:JuliaExpertAsk</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; ask</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;xyz&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;abc&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, api_kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> run!</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Is equivalent to</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:JuliaExpertAsk</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; ask</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;xyz&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;abc&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, api_kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), return_all</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># The only difference is that we default to `return_all=true` with lazy types because we have a dedicated `conversation` field, which makes it much easier</span></span></code></pre></div><p>Lazy AI calls and self-healing mechanisms unlock much more robust and useful LLM workflows!</p><h2 id="Walkthrough-Example-for-aigenerate" tabindex="-1">Walkthrough Example for <code>aigenerate</code> <a class="header-anchor" href="#Walkthrough-Example-for-aigenerate" aria-label="Permalink to &quot;Walkthrough Example for `aigenerate` {#Walkthrough-Example-for-aigenerate}&quot;">​</a></h2><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Let&#39;s say this is our ask</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:AssistantAsk</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; ask</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What is the capital of France?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># it is effectively the same as:</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OpenAISchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(), PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AITemplate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:AssistantAsk</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">); ask</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What is the capital of France?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt3t&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>There is no <code>model</code> provided, so we use the default <code>PT.MODEL_CHAT</code> (effectively GPT3.5-Turbo). Then we look it up in <code>PT.MDOEL_REGISTRY</code> and use the associated schema for it (<code>OpenAISchema</code> in this case).</p><p>The next step is to render the template, replace the placeholders and render it for the OpenAI model.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Let&#39;s remember out schema</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">schema </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OpenAISchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ask </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;What is the capital of France?&quot;</span></span></code></pre></div><p>First, we obtain the template (no placeholder replacement yet) and &quot;expand it&quot;</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">template_rendered </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">render</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(schema, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AITemplate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:AssistantAsk</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">); ask)</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>2-element Vector{PromptingTools.AbstractChatMessage}:</span></span>\n<span class="line"><span>  PromptingTools.SystemMessage(&quot;You are a world-class AI assistant. Your communication is brief and concise. You&#39;re precise and answer only when you&#39;re confident in the high quality of your answer.&quot;)</span></span>\n<span class="line"><span>  PromptingTools.UserMessage{String}(&quot;# Question\\n\\n{{ask}}&quot;, [:ask], :usermessage)</span></span></code></pre></div><p>Second, we replace the placeholders</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">rendered_for_api </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">render</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(schema, template_rendered;  ask)</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>2-element Vector{Dict{String, Any}}:</span></span>\n<span class="line"><span>  Dict(&quot;role&quot; =&gt; &quot;system&quot;, &quot;content&quot; =&gt; &quot;You are a world-class AI assistant. Your communication is brief and concise. You&#39;re precise and answer only when you&#39;re confident in the high quality of your answer.&quot;)</span></span>\n<span class="line"><span>  Dict(&quot;role&quot; =&gt; &quot;user&quot;, &quot;content&quot; =&gt; &quot;# Question\\n\\nWhat is the capital of France?&quot;)</span></span></code></pre></div><p>Notice that the placeholders are only replaced in the second step. The final output here is a vector of messages with &quot;role&quot; and &quot;content&quot; keys, which is the format required by the OpenAI API.</p><p>As a side note, under the hood, the second step is done in two sub-steps:</p><ul><li><p>replace the placeholders <code>messages_rendered = PT.render(PT.NoSchema(), template_rendered; ask)</code> -&gt; returns a vector of Messages!</p></li><li><p>then, we convert the messages to the format required by the provider/schema <code>PT.render(schema, messages_rendered)</code> -&gt; returns the OpenAI formatted messages</p></li></ul><p>Next, we send the above <code>rendered_for_api</code> to the OpenAI API and get the response back.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">OpenAI</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">create_chat</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(api_key, model, rendered_for_api)</span></span></code></pre></div><p>The last step is to take the JSON response from the API and convert it to the <code>AIMessage</code> object.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># simplification for educational purposes</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AIMessage</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> r</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:choices</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:message</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:content</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre></div><p>In practice, there are more fields we extract, so we define a utility for it: <code>PT.response_to_message</code>. Especially, since with parameter <code>n</code>, you can request multiple AI responses at once, so we want to re-use our response processing logic.</p><p>That&#39;s it! I hope you&#39;ve learned something new about how PromptingTools.jl works under the hood.</p><h2 id="Walkthrough-Example-for-aiextract" tabindex="-1">Walkthrough Example for <code>aiextract</code> <a class="header-anchor" href="#Walkthrough-Example-for-aiextract" aria-label="Permalink to &quot;Walkthrough Example for `aiextract` {#Walkthrough-Example-for-aiextract}&quot;">​</a></h2><p>Whereas <code>aigenerate</code> is a general-purpose function to generate any text response with LLMs, <code>aiextract</code> is designed to extract structured data from the AI model&#39;s response and return them as a Julia struct.</p><p>It&#39;s a bit more complicated than <code>aigenerate</code> because it needs to handle the JSON schema of the return type (= our struct).</p><p>Let&#39;s define a toy example of a struct and see how <code>aiextract</code> works under the hood.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Extract the name of the food from the sentence. Extract any provided adjectives for the food as well.</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Example: &quot;I am eating a crunchy bread.&quot; -&gt; Food(&quot;bread&quot;, [&quot;crunchy&quot;])</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;&quot;</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Food</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">String</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # required field!</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    adjectives</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Union{Nothing,Vector{String}}</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # not required because `Nothing` is allowed</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aiextract</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;I just ate a delicious and juicy apple.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; return_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Food)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Food(&quot;apple&quot;, [&quot;delicious&quot;, &quot;juicy&quot;])</span></span></code></pre></div><p>You can see that we sent a prompt to the AI model and it returned a <code>Food</code> object. We provided some light guidance as a docstring of the return type, but the AI model did the heavy lifting.</p><p><code>aiextract</code> leverages native &quot;function calling&quot; (supported by OpenAI, Fireworks, Together, and many others).</p><p>We encode the user-provided <code>return_type</code> into the corresponding JSON schema and create the payload as per the specifications of the provider.</p><p>Let&#39;s how that&#39;s done:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sig </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">function_call_signature</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Food)</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## Dict{String, Any} with 3 entries:</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   &quot;name&quot;        =&gt; &quot;Food_extractor&quot;</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   &quot;parameters&quot;  =&gt; Dict{String, Any}(&quot;properties&quot;=&gt;Dict{String, Any}(&quot;name&quot;=&gt;Dict(&quot;type&quot;=&gt;&quot;string&quot;), &quot;adjectives&quot;=&gt;Dict{String, …</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   &quot;description&quot; =&gt; &quot;Extract the food from the sentence. Extract any provided adjectives for the food as well.\\n\\nExample: &quot;</span></span></code></pre></div><p>You can see that we capture the field names and types in <code>parameters</code> and the description in <code>description</code> key.</p><p>Furthermore, if we zoom in on the &quot;parameter&quot; field, you can see that we encode not only the names and types but also whether the fields are required (ie, do they allow <code>Nothing</code>) You can see below that the field <code>adjectives</code> accepts <code>Nothing</code>, so it&#39;s not required. Only the <code>name</code> field is required.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sig[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;parameters&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## Dict{String, Any} with 3 entries:</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   &quot;properties&quot; =&gt; Dict{String, Any}(&quot;name&quot;=&gt;Dict(&quot;type&quot;=&gt;&quot;string&quot;), &quot;adjectives&quot;=&gt;Dict{String, Any}(&quot;items&quot;=&gt;Dict(&quot;type&quot;=&gt;&quot;strin…</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   &quot;required&quot;   =&gt; [&quot;name&quot;]</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   &quot;type&quot;       =&gt; &quot;object&quot;</span></span></code></pre></div><p>For <code>aiextract</code>, the signature is provided to the API provider via <code>tools</code> parameter, eg,</p><p><code>api_kwargs = (; tools = [Dict(:type =&gt; &quot;function&quot;, :function =&gt; sig)])</code></p><p>Optionally, we can provide also <code>tool_choice</code> parameter to specify which tool to use if we provided multiple (differs across providers).</p><p>When the message is returned, we extract the JSON object in the response and decode it into Julia object via <code>JSON3.read(obj, Food)</code>. For example,</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:tool_calls</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:function</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:arguments</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">write</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;name&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;apple&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;adjectives&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;delicious&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;juicy&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]))))])</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">food </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">read</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(model_response[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:tool_calls</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:arguments</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], Food)</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Output: Food(&quot;apple&quot;, [&quot;delicious&quot;, &quot;juicy&quot;])</span></span></code></pre></div><p>This is why you can sometimes have errors when you use abstract types in your <code>return_type</code> -&gt; to enable that, you would need to set the right <code>StructTypes</code> behavior for your abstract type (see the JSON3.jl documentation for more details on how to do that).</p><p>It works quite well for concrete types and &quot;vanilla&quot; structs, though.</p><p>Unfortunately, function calling is generally NOT supported by locally-hosted / open-source models, so let&#39;s try to build a workaround with <code>aigenerate</code></p><p>You need to pick a bigger / more powerful model, as it&#39;s NOT an easy task to output a correct JSON specification. My laptop isn&#39;t too powerful and I don&#39;t like waiting, so I&#39;m going to use Mixtral model hosted on Together.ai (you get $25 credit when you join)!</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;tmixtral&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # tmixtral is an alias for &quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot; on Together.ai and it automatically sets `schema = TogetherOpenAISchema()`</span></span></code></pre></div><p>We&#39;ll add the signature to the prompt and we&#39;ll request the JSON output in two places - in the prompt and in the <code>api_kwargs</code> (to ensure that the model outputs the JSON via &quot;grammar&quot;) NOTE: You can write much better and more specific prompt if you have a specific task / return type in mind + you should make sure that the prompt + struct description make sense together!</p>', 64);
const _hoisted_93 = /* @__PURE__ */ createBaseVNode("code", null, "return_type", -1);
const _hoisted_94 = /* @__PURE__ */ createStaticVNode('<div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;&quot;&quot;</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">You&#39;re a world-class data extraction engine. </span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Your task is to extract information formatted as per the user provided schema.</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">You MUST response in JSON format.</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">**Example:**</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">---------</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Description: &quot;Extract the Car from the sentence. Extract the corresponding brand and model as well.&quot;</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Input: &quot;I drive a black Porsche 911 Turbo.&quot;</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Schema: &quot;{</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">properties</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:{</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">model</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:{</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">type</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">string</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">},</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">brand</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:{</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">type</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">string</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}},</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">required</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">brand</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">model</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">],</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">type</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">object</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}&quot;</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Output: &quot;{</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">model</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">911 Turbo</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">brand</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Porsche</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}&quot;</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">---------</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">**User Request:**</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Description: {{description}}</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Input: {{input}}</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Schema: {{signature}}</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Output:</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">You MUST OUTPUT in JSON format.</span></span>\n<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;&quot;</span></span></code></pre></div><p>We need to extract the &quot;signature of our <code>return_type</code> and put it in the right placeholders. Let&#39;s generate now!</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sig </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">function_call_signature</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Food)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prompt; input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;I just ate a delicious and juicy apple.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    schema</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">write</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(sig[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;parameters&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]), description</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sig[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;description&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    ## We provide the JSON output requirement as per API docs: https://docs.together.ai/docs/json-mode</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model, api_kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; response_format</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;type&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;json_object&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), return_all</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">end</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## &quot;{\\n  \\&quot;adjectives\\&quot;: [\\&quot;delicious\\&quot;, \\&quot;juicy\\&quot;],\\n  \\&quot;food\\&quot;: \\&quot;apple\\&quot;\\n}&quot;</span></span></code></pre></div><p>We&#39;re using a smaller model, so the output is not perfect. Let&#39;s try to load into our object:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">obj </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">read</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(result[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">end</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content, Food)</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Output: ERROR: MethodError: Cannot `convert` an object of type Nothing to an object of type String</span></span></code></pre></div><p>Unfortunately, we get an error because the model mixed up the key &quot;name&quot; for &quot;food&quot;, so it cannot be parsed.</p><p>Fortunately, we can do better and use automatic fixing! All we need to do is to change from <code>aigenerate</code> -&gt; <code>AIGenerate</code> (and use <code>airetry!</code>)</p><p>The signature of <code>AIGenerate</code> is identical to <code>aigenerate</code> with the exception of <code>config</code> field, where we can influence the future <code>retry</code> behaviour.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AIGenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prompt; input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;I just ate a delicious and juicy apple.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    schema</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">write</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(sig[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;parameters&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]), description</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sig[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;description&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    ## We provide the JSON output requirement as per API docs: https://docs.together.ai/docs/json-mode</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model, api_kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; response_format</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;type&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;json_object&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    ## limit the number of retries, default is 10 rounds</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    config</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">RetryConfig</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; max_retries</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">run!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(result) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># run! triggers the generation step (to have some AI output to check)</span></span></code></pre></div><p>Let&#39;s set up a retry mechanism with some practical feedback. We&#39;ll leverage <code>airetry!</code> to automatically retry the request and provide feedback to the model. Think of <code>airetry!</code> as <code>@assert</code> on steroids:</p><p><code>@assert CONDITION MESSAGE</code> → <code>airetry! CONDITION &lt;state&gt; MESSAGE</code></p><p>The main benefits of <code>airetry!</code> are:</p><ul><li><p>It can retry automatically, not just throw an error</p></li><li><p>It manages the &quot;conversation’ (list of messages) for you, including adding user-provided feedback to help generate better output</p></li></ul><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">feedback </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;The output is not in the correct format. The keys should be </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">$(join([string(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\&quot;$f\\&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> f </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> fieldnames(Food)],</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;, &quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">))</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">.&quot;</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We use do-syntax with provide the `CONDITION` (it must return Bool)</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">airetry!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(result, feedback) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">do</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> conv</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    ## try to convert</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    obj </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> try</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">read</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">last_output</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(conv), Food)</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    catch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> e</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        ## you could save the error and provide as feedback (eg, into a slot in the `:memory` field of the AICall object)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        e</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    ## Check if the conversion was successful; if it&#39;s `false`, it will retry</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    obj </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">isa</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Food </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># -&gt; Bool</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">food </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> JSON3</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">read</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">last_output</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(result), Food)</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## [ Info: Condition not met. Retrying...</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## Output: Food(&quot;apple&quot;, [&quot;delicious&quot;, &quot;juicy&quot;])</span></span></code></pre></div><p>It took 1 retry (see <code>result.config.retries</code>) and we have the correct output from an open-source model!</p><p>If you&#39;re interested in the <code>result</code> object, it&#39;s a struct (<code>AICall</code>) with a field <code>conversation</code>, which holds the conversation up to this point. AIGenerate is an alias for AICall using <code>aigenerate</code> function. See <code>?AICall</code> (the underlying struct type) for more details on the fields and methods available.</p>', 16);
function _sfc_render(_ctx, _cache, $props, $setup, $data, $options) {
  return openBlock(), createElementBlock("div", null, [
    _hoisted_1,
    createBaseVNode("p", null, [
      createTextVNode('We want to have re-usable "prompts", so we provide you with a system to retrieve pre-defined prompts with placeholders (eg, '),
      createBaseVNode("code", null, toDisplayString(_ctx.name), 1),
      createTextVNode(") that you can replace with your inputs at the time of making the request.")
    ]),
    _hoisted_25,
    createBaseVNode("p", null, [
      createTextVNode("Notice that we have a placeholder "),
      _hoisted_28,
      createTextVNode(" ("),
      createBaseVNode("code", null, toDisplayString(_ctx.ask), 1),
      createTextVNode(") that you can replace with your question without having to re-write the generic system instructions.")
    ]),
    _hoisted_29,
    createBaseVNode("p", null, [
      createTextVNode("Let's define a prompt and "),
      _hoisted_93,
      createTextVNode(". Notice that we add several placeholders (eg, "),
      createBaseVNode("code", null, toDisplayString(_ctx.description), 1),
      createTextVNode(") to fill with user inputs later.")
    ]),
    _hoisted_94
  ]);
}
const how_it_works = /* @__PURE__ */ _export_sfc(_sfc_main, [["render", _sfc_render]]);
export {
  __pageData,
  how_it_works as default
};
