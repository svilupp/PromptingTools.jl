<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Frequently Asked Questions | PromptingTools.jl</title>
    <meta name="description" content="Streamline Your Interactions with GenAI Models. Discover the power of GenerativeAI and build mini workflows to save you 20 minutes every day.">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/PromptingTools.jl/dev/assets/style.DHDbEIxu.css" as="style">
    <link rel="preload stylesheet" href="/PromptingTools.jl/dev/vp-icons.css" as="style">
    
    <script type="module" src="/PromptingTools.jl/dev/assets/app.CPcYLVLj.js"></script>
    <link rel="preload" href="/PromptingTools.jl/dev/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/PromptingTools.jl/dev/assets/chunks/theme.Bdt6cMbh.js">
    <link rel="modulepreload" href="/PromptingTools.jl/dev/assets/chunks/framework.C0CpARbg.js">
    <link rel="modulepreload" href="/PromptingTools.jl/dev/assets/frequently_asked_questions.md.DAORlXGh.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/PromptingTools.jl/dev/" data-v-0f4f798b><!--[--><!--]--><!--[--><img class="VPImage logo" src="/PromptingTools.jl/dev/logo.png" width="24" height="24" alt data-v-35a7d0b8><!--]--><span data-v-0f4f798b>PromptingTools.jl</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/PromptingTools.jl/dev/index" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Manual</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/getting_started" data-v-acbfed09><!--[--><span data-v-acbfed09>Getting Started</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/how_it_works" data-v-acbfed09><!--[--><span data-v-acbfed09>How It Works</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/coverage_of_model_providers" data-v-acbfed09><!--[--><span data-v-acbfed09>Coverage of Model Providers</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Examples</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/examples/readme_examples" data-v-acbfed09><!--[--><span data-v-acbfed09>Various examples</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/examples/working_with_aitemplates" data-v-acbfed09><!--[--><span data-v-acbfed09>Using AITemplates</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/examples/working_with_ollama" data-v-acbfed09><!--[--><span data-v-acbfed09>Local models with Ollama.ai</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/examples/working_with_google_ai_studio" data-v-acbfed09><!--[--><span data-v-acbfed09>Google AIStudio</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/examples/working_with_custom_apis" data-v-acbfed09><!--[--><span data-v-acbfed09>Custom APIs (Mistral, Llama.cpp)</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="VPMenuGroup" data-v-7dd3104a data-v-48c802d0><p class="title" data-v-48c802d0>Extra Tools</p><!--[--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/extra_tools/text_utilities_intro" data-v-acbfed09><!--[--><span data-v-acbfed09>Text Utilities</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/extra_tools/agent_tools_intro" data-v-acbfed09><!--[--><span data-v-acbfed09>AgentTools</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/extra_tools/rag_tools_intro" data-v-acbfed09><!--[--><span data-v-acbfed09>RAGTools</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/ragtools_migration" data-v-acbfed09><!--[--><span data-v-acbfed09>RAGTools Migration</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-48c802d0 data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/extra_tools/api_tools_intro" data-v-acbfed09><!--[--><span data-v-acbfed09>APITools</span><!--]--></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/PromptingTools.jl/dev/frequently_asked_questions" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>F.A.Q.</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Prompt Templates</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/general" data-v-acbfed09><!--[--><span data-v-acbfed09>General</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/persona-task" data-v-acbfed09><!--[--><span data-v-acbfed09>Persona-Task</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/visual" data-v-acbfed09><!--[--><span data-v-acbfed09>Visual</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/classification" data-v-acbfed09><!--[--><span data-v-acbfed09>Classification</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/extraction" data-v-acbfed09><!--[--><span data-v-acbfed09>Extraction</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/agents" data-v-acbfed09><!--[--><span data-v-acbfed09>Agents</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/prompts/RAG" data-v-acbfed09><!--[--><span data-v-acbfed09>RAG</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Reference</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/reference" data-v-acbfed09><!--[--><span data-v-acbfed09>PromptingTools.jl</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/reference_experimental" data-v-acbfed09><!--[--><span data-v-acbfed09>Experimental Modules</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/reference_agenttools" data-v-acbfed09><!--[--><span data-v-acbfed09>AgentTools</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/PromptingTools.jl/dev/reference_apitools" data-v-acbfed09><!--[--><span data-v-acbfed09>APITools</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/svilupp/PromptingTools.jl" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/svilupp/PromptingTools.jl" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/index" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Home</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Manual</h2><!----></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/getting_started" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Getting Started</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/how_it_works" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>How It Works</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/coverage_of_model_providers" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Coverage of Model Providers</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Examples</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/examples/readme_examples" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Various examples</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/examples/working_with_aitemplates" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Using AITemplates</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/examples/working_with_ollama" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Local models with Ollama.ai</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/examples/working_with_google_ai_studio" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Google AIStudio</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/examples/working_with_custom_apis" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Custom APIs (Mistral, Llama.cpp)</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible collapsed" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h3 class="text" data-v-a4b0d9bf>Extra Tools</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/extra_tools/text_utilities_intro" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Text Utilities</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/extra_tools/agent_tools_intro" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>AgentTools</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/extra_tools/api_tools_intro" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>APITools</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 has-active" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/frequently_asked_questions" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>F.A.Q.</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible collapsed" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Prompt Templates</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/general" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>General</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/persona-task" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Persona-Task</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/visual" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Visual</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/classification" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Classification</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/extraction" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Extraction</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/agents" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Agents</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/prompts/RAG" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>RAG</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible collapsed" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Reference</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/reference" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>PromptingTools.jl</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/reference_experimental" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Experimental Modules</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/reference_agenttools" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>AgentTools</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/PromptingTools.jl/dev/reference_apitools" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>APITools</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _PromptingTools_jl_dev_frequently_asked_questions" data-v-83890dd9><div><h1 id="Frequently-Asked-Questions" tabindex="-1">Frequently Asked Questions <a class="header-anchor" href="#Frequently-Asked-Questions" aria-label="Permalink to &quot;Frequently Asked Questions {#Frequently-Asked-Questions}&quot;">​</a></h1><h2 id="Why-OpenAI" tabindex="-1">Why OpenAI <a class="header-anchor" href="#Why-OpenAI" aria-label="Permalink to &quot;Why OpenAI {#Why-OpenAI}&quot;">​</a></h2><p>OpenAI&#39;s models are at the forefront of AI research and provide robust, state-of-the-art capabilities for many tasks.</p><p>There will be situations not or cannot use it (eg, privacy, cost, etc.). In that case, you can use local models (eg, Ollama) or other APIs (eg, Anthropic).</p><p>Note: To get started with <a href="https://ollama.ai/" target="_blank" rel="noreferrer">Ollama.ai</a>, see the <a href="/PromptingTools.jl/dev/frequently_asked_questions#setup-guide-for-ollama">Setup Guide for Ollama</a> section below.</p><h3 id="What-if-I-cannot-access-OpenAI?" tabindex="-1">What if I cannot access OpenAI? <a class="header-anchor" href="#What-if-I-cannot-access-OpenAI?" aria-label="Permalink to &quot;What if I cannot access OpenAI? {#What-if-I-cannot-access-OpenAI?}&quot;">​</a></h3><p>There are many alternatives:</p><ul><li><p><strong>Other APIs</strong>: MistralAI, Anthropic, Google, Together, Fireworks, Voyager (the latter ones tend to give free credits upon joining!)</p></li><li><p><strong>Locally-hosted models</strong>: Llama.cpp/Llama.jl, Ollama, vLLM (see the examples and the corresponding docs)</p></li></ul><h2 id="Data-Privacy-and-OpenAI" tabindex="-1">Data Privacy and OpenAI <a class="header-anchor" href="#Data-Privacy-and-OpenAI" aria-label="Permalink to &quot;Data Privacy and OpenAI {#Data-Privacy-and-OpenAI}&quot;">​</a></h2><p>At the time of writing, OpenAI does NOT use the API calls for training their models.</p><blockquote><p><strong>API</strong></p><p>OpenAI does not use data submitted to and generated by our API to train OpenAI models or improve OpenAI’s service offering. In order to support the continuous improvement of our models, you can fill out this form to opt-in to share your data with us. – <a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance" target="_blank" rel="noreferrer">How your data is used to improve our models</a></p></blockquote><p>You can always double-check the latest information on the <a href="https://platform.openai.com/docs/models/how-we-use-your-data" target="_blank" rel="noreferrer">OpenAI&#39;s How we use your data</a> page.</p><p>Resources:</p><ul><li><p><a href="https://platform.openai.com/docs/models/how-we-use-your-data" target="_blank" rel="noreferrer">OpenAI&#39;s How we use your data</a></p></li><li><p><a href="https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq" target="_blank" rel="noreferrer">Data usage for consumer services FAQ</a></p></li><li><p><a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance" target="_blank" rel="noreferrer">How your data is used to improve our models</a></p></li></ul><h2 id="Creating-OpenAI-API-Key" tabindex="-1">Creating OpenAI API Key <a class="header-anchor" href="#Creating-OpenAI-API-Key" aria-label="Permalink to &quot;Creating OpenAI API Key {#Creating-OpenAI-API-Key}&quot;">​</a></h2><p>You can get your API key from OpenAI by signing up for an account and accessing the API section of the OpenAI website.</p><ol><li><p>Create an account with <a href="https://platform.openai.com/signup" target="_blank" rel="noreferrer">OpenAI</a></p></li><li><p>Go to <a href="https://platform.openai.com/account/api-keys" target="_blank" rel="noreferrer">API Key page</a></p></li><li><p>Click on “Create new secret key”</p></li></ol><p>!!! Do not share it with anyone and do NOT save it to any files that get synced online.</p><p>Resources:</p><ul><li><p><a href="https://platform.openai.com/docs/quickstart?context=python" target="_blank" rel="noreferrer">OpenAI Documentation</a></p></li><li><p><a href="https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt" target="_blank" rel="noreferrer">Visual tutorial</a></p></li></ul><p>Pro tip: Always set the spending limits!</p><h2 id="getting-an-error-argumenterror-api-key-cannot-be-empty-despite-having-set-openai-api-key-getting-an-error-argumenterror-apikey-cannot-be-empty-despite-having-set-openaiapi-key" tabindex="-1">Getting an error &quot;ArgumentError: api_key cannot be empty&quot; despite having set <code>OPENAI_API_KEY</code>? {#Getting-an-error-&quot;ArgumentError:-api<em>key-cannot-be-empty&quot;-despite-having-set-OPENAI</em>API_KEY?} <a class="header-anchor" href="#getting-an-error-argumenterror-api-key-cannot-be-empty-despite-having-set-openai-api-key-getting-an-error-argumenterror-apikey-cannot-be-empty-despite-having-set-openaiapi-key" aria-label="Permalink to &quot;Getting an error &quot;ArgumentError: api_key cannot be empty&quot; despite having set `OPENAI_API_KEY`? {#Getting-an-error-&quot;ArgumentError:-api*key-cannot-be-empty&quot;-despite-having-set-OPENAI*API_KEY?}&quot;">​</a></h2><p>Quick fix: just provide kwarg <code>api_key</code> with your key to the <code>aigenerate</code> function (and other <code>ai*</code> functions).</p><p>This error is thrown when the OpenAI API key is not available in 1) local preferences or 2) environment variables (<code>ENV[&quot;OPENAI_API_KEY&quot;]</code>).</p><p>First, check if you can access the key by running <code>ENV[&quot;OPENAI_API_KEY&quot;]</code> in the Julia REPL. If it returns <code>nothing</code>, the key is not set.</p><p>If the key is set, but you still get the error, there was a rare bug in earlier versions where if you first precompiled PromptingTools without the API key, it would remember it and &quot;compile away&quot; the <code>get(ENV,...)</code> function call. If you&#39;re experiencing this bug on the latest version of PromptingTools, please open an issue on GitHub.</p><p>The solution is to force a new precompilation, so you can do any of the below:</p><ol><li><p>Force precompilation (run <code>Pkg.precompile()</code> in the Julia REPL)</p></li><li><p>Update the PromptingTools package (runs precompilation automatically)</p></li><li><p>Delete your compiled cache in <code>.julia</code> DEPOT (usually <code>.julia/compiled/v1.10/PromptingTools</code>). You can do it manually in the file explorer or via Julia REPL: <code>rm(&quot;~/.julia/compiled/v1.10/PromptingTools&quot;, recursive=true, force=true)</code></p></li></ol><h2 id="Getting-an-error-&quot;Rate-limit-exceeded&quot;-from-OpenAI?" tabindex="-1">Getting an error &quot;Rate limit exceeded&quot; from OpenAI? <a class="header-anchor" href="#Getting-an-error-&quot;Rate-limit-exceeded&quot;-from-OpenAI?" aria-label="Permalink to &quot;Getting an error &quot;Rate limit exceeded&quot; from OpenAI? {#Getting-an-error-&quot;Rate-limit-exceeded&quot;-from-OpenAI?}&quot;">​</a></h2><p>Have you opened a new account recently? It is quite likely that you&#39;ve exceeded the free tier limits.</p><p>OpenAI has a rate limit on the number of requests and the number of tokens you can make in a given period. If you exceed either of these, you will receive a &quot;Rate limit exceeded&quot; error. &quot;Free tier&quot; (ie, before you pay the first 5 USD) has very low limits, eg, maximum of 3 requests per minute. See the <a href="https://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-free" target="_blank" rel="noreferrer">OpenAI Rate Limits</a> for more information.</p><p>If you look at the HTTP response headers in the error, you can see the limits remaining and how long until it resets, eg, <code>x-ratelimit-remaining-*</code> and <code>x-ratelimit-reset-*</code>.</p><p>If you want to avoid this error, you have two options:</p><ol><li><p>Put a simple <code>sleep(x)</code> after every request, where <code>x</code> is calculated so that the number of your requests stays below the limit.</p></li><li><p>Use <code>ntasks</code> keyword argument in <code>asyncmap</code> to limit the number of concurrent requests. Eg, let&#39;s assume you want to process 100x c. 10,000 tokens, but your tier limit is only 60,000 tokens per minute. If we know that one request takes c. 10 seconds, it means that with <code>ntasks=1</code> we would send 6 requests per minute, which already maxes out our limit. If we set <code>ntasks=2</code>, we could process 12 requests per minute, so we would need our limit to be 120,000 tokens per minute.</p></li></ol><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># simple asyncmap loop with 2 concurrent requests; otherwise, same syntax as `map`</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">asyncmap</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(my_prompts; ntasks</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">do</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> prompt</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prompt)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><h2 id="Getting-the-error-&quot;429-Too-Many-Requests&quot;?" tabindex="-1">Getting the error &quot;429 Too Many Requests&quot;? <a class="header-anchor" href="#Getting-the-error-&quot;429-Too-Many-Requests&quot;?" aria-label="Permalink to &quot;Getting the error &quot;429 Too Many Requests&quot;? {#Getting-the-error-&quot;429-Too-Many-Requests&quot;?}&quot;">​</a></h2><p>Assuming you have not just sent hundreds of requests, this error might be related to insufficient &quot;credits&quot; in your account balance.</p><p>See the error message. If it says &quot;You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: <a href="https://platform.openai.com/docs/guides/error-codes/api-errors" target="_blank" rel="noreferrer">https://platform.openai.com/docs/guides/error-codes/api-errors</a>&quot;, you&#39;ll need to re-charge your account balance. Visit <a href="https://platform.openai.com/settings/organization/billing/overview" target="_blank" rel="noreferrer">Billing overview</a>.</p><p>Please note that, unlike ChatGPT, OpenAI API is NOT free. However, individual requests are extremely cheap (eg, tenth of a cent), so if you charge 5 , it might last you up to hundreds of requests (depending on the models and prompts).</p><h2 id="Setting-OpenAI-Spending-Limits" tabindex="-1">Setting OpenAI Spending Limits <a class="header-anchor" href="#Setting-OpenAI-Spending-Limits" aria-label="Permalink to &quot;Setting OpenAI Spending Limits {#Setting-OpenAI-Spending-Limits}&quot;">​</a></h2><p>OpenAI allows you to set spending limits directly on your account dashboard to prevent unexpected costs.</p><ol><li><p>Go to <a href="https://platform.openai.com/account/billing" target="_blank" rel="noreferrer">OpenAI Billing</a></p></li><li><p>Set Soft Limit (you’ll receive a notification) and Hard Limit (API will stop working not to spend more money)</p></li></ol><p>A good start might be a soft limit of c.$5 and a hard limit of c.$10 - you can always increase it later in the month.</p><p>Resources:</p><ul><li><a href="https://community.openai.com/t/how-to-set-a-price-limit/13086" target="_blank" rel="noreferrer">OpenAI Forum</a></li></ul><h2 id="How-much-does-it-cost?-Is-it-worth-paying-for?" tabindex="-1">How much does it cost? Is it worth paying for? <a class="header-anchor" href="#How-much-does-it-cost?-Is-it-worth-paying-for?" aria-label="Permalink to &quot;How much does it cost? Is it worth paying for? {#How-much-does-it-cost?-Is-it-worth-paying-for?}&quot;">​</a></h2><p>If you use a local model (eg, with Ollama), it&#39;s free. If you use any commercial APIs (eg, OpenAI), you will likely pay per &quot;token&quot; (a sub-word unit).</p><p>For example, a simple request with a simple question and 1 sentence response in return (”Is statement XYZ a positive comment”) will cost you ~0.0001 (ie, one-hundredth of a cent)</p><p><strong>Is it worth paying for?</strong></p><p>GenAI is a way to buy time! You can pay cents to save tens of minutes every day.</p><p>Continuing the example above, imagine you have a table with 200 comments. Now, you can parse each one of them with an LLM for the features/checks you need. Assuming the price per call was 0.0001 , you&#39;d pay 2 cents for the job and save 30-60 minutes of your time!</p><p>Resources:</p><ul><li><a href="https://openai.com/pricing" target="_blank" rel="noreferrer">OpenAI Pricing per 1000 tokens</a></li></ul><h2 id="How-to-try-new-OpenAI-models-if-I&#39;m-not-Tier-5-customer?" tabindex="-1">How to try new OpenAI models if I&#39;m not Tier 5 customer? <a class="header-anchor" href="#How-to-try-new-OpenAI-models-if-I&#39;m-not-Tier-5-customer?" aria-label="Permalink to &quot;How to try new OpenAI models if I&#39;m not Tier 5 customer? {#How-to-try-new-OpenAI-models-if-I&#39;m-not-Tier-5-customer?}&quot;">​</a></h2><p>As of September 2024, you cannot access the new o1 models via API unless you&#39;re a Tier 5 customer.</p><p>Fortunately, you can use OpenRouter to access these new models.</p><ol><li><p>Get your API key from <a href="https://openrouter.ai/keys" target="_blank" rel="noreferrer">OpenRouter</a></p></li><li><p>Add some minimum <a href="https://openrouter.ai/credits" target="_blank" rel="noreferrer">Credits</a> to the account (eg, 5 ).</p></li><li><p>Set it as an environment variable (or use local preferences): <code>ENV[&quot;OPENROUTER_API_KEY&quot;] = &quot;&lt;your key&gt;&quot;</code></p></li><li><p>Use the model aliases with <code>or</code> prefix, eg, <code>oro1</code> for o1-preview or <code>oro1m</code> for o1-mini.</p></li></ol><p>Example:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Let&#39;s use o1-preview model hosted on OpenRouter (&quot;or&quot; prefix)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What is the meaning of life?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;oro1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Note: There are some quirks for the o1 models. For example, the new o1 series does NOT support <code>SystemMessage</code> yet, so OpenRouter does some tricks (likely converting them to normal user messages). To be in control of this behavior and have comparable behavior to the native OpenAI API, you can use kwarg <code>no_system_message=true</code> in <code>aigenerate</code> to ensure OpenRouter does not do any tricks.</p><p>Example:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Let&#39;s use o1-mini and disable adding automatic system message</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What is the meaning of life?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;oro1m&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, no_system_message</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h2 id="Configuring-the-Environment-Variable-for-API-Key" tabindex="-1">Configuring the Environment Variable for API Key <a class="header-anchor" href="#Configuring-the-Environment-Variable-for-API-Key" aria-label="Permalink to &quot;Configuring the Environment Variable for API Key {#Configuring-the-Environment-Variable-for-API-Key}&quot;">​</a></h2><p>This is a guide for OpenAI&#39;s API key, but it works for any other API key you might need (eg, <code>MISTRAL_API_KEY</code> for MistralAI API).</p><p>To use the OpenAI API with PromptingTools.jl, set your API key as an environment variable:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">ENV</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;OPENAI_API_KEY&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;your-api-key&quot;</span></span></code></pre></div><p>As a one-off, you can:</p><ul><li><p>set it in the terminal before launching Julia: <code>export OPENAI_API_KEY = &lt;your key&gt;</code></p></li><li><p>set it in your <code>setup.jl</code> (make sure not to commit it to GitHub!)</p></li></ul><p>Make sure to start Julia from the same terminal window where you set the variable. Easy check in Julia, run <code>ENV[&quot;OPENAI_API_KEY&quot;]</code> and you should see your key!</p><p>A better way:</p><ul><li><p>On a Mac, add the configuration line to your terminal&#39;s configuration file (eg, <code>~/.zshrc</code>). It will get automatically loaded every time you launch the terminal</p></li><li><p>On Windows, set it as a system variable in &quot;Environment Variables&quot; settings (see the Resources)</p></li></ul><p>Resources:</p><ul><li><a href="https://platform.openai.com/docs/quickstart?context=python" target="_blank" rel="noreferrer">OpenAI Guide</a></li></ul><h2 id="Setting-the-API-Key-via-Preferences.jl" tabindex="-1">Setting the API Key via Preferences.jl <a class="header-anchor" href="#Setting-the-API-Key-via-Preferences.jl" aria-label="Permalink to &quot;Setting the API Key via Preferences.jl {#Setting-the-API-Key-via-Preferences.jl}&quot;">​</a></h2><p>You can also set the API key in <code>LocalPreferences.toml</code>, so it persists across sessions and projects.</p><p>Use: <code>PromptingTools.set_preferences!(&quot;OPENAI_API_KEY&quot;=&gt;&quot;your-api-key&quot;)</code></p><p>To double-check, run <code>PromptingTools.get_preferences(&quot;OPENAI_API_KEY&quot;)</code> and you should see your key!</p><p>See more detail in the <code>?PromptingTools.PREFERENCES</code> docstring.</p><h2 id="Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)" tabindex="-1">Understanding the API Keyword Arguments in <code>aigenerate</code> (<code>api_kwargs</code>) <a class="header-anchor" href="#Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)" aria-label="Permalink to &quot;Understanding the API Keyword Arguments in `aigenerate` (`api_kwargs`) {#Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)}&quot;">​</a></h2><p>See <a href="https://platform.openai.com/docs/guides/text-generation/chat-completions-api" target="_blank" rel="noreferrer">OpenAI API reference</a> for more information.</p><h2 id="Instant-Access-from-Anywhere" tabindex="-1">Instant Access from Anywhere <a class="header-anchor" href="#Instant-Access-from-Anywhere" aria-label="Permalink to &quot;Instant Access from Anywhere {#Instant-Access-from-Anywhere}&quot;">​</a></h2><p>For easy access from anywhere, add PromptingTools into your <code>startup.jl</code> (can be found in <code>~/.julia/config/startup.jl</code>).</p><p>Add the following snippet:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>using PromptingTools</span></span>
<span class="line"><span>const PT = PromptingTools # to access unexported functions and types</span></span></code></pre></div><p>Now, you can just use <code>ai&quot;Help me do X to achieve Y&quot;</code> from any REPL session!</p><h2 id="Open-Source-Alternatives" tabindex="-1">Open Source Alternatives <a class="header-anchor" href="#Open-Source-Alternatives" aria-label="Permalink to &quot;Open Source Alternatives {#Open-Source-Alternatives}&quot;">​</a></h2><p>The ethos of PromptingTools.jl is to allow you to use whatever model you want, which includes Open Source LLMs. The most popular and easiest to setup is <a href="https://ollama.ai/" target="_blank" rel="noreferrer">Ollama.ai</a> - see below for more information.</p><h2 id="Setup-Guide-for-Ollama" tabindex="-1">Setup Guide for Ollama <a class="header-anchor" href="#Setup-Guide-for-Ollama" aria-label="Permalink to &quot;Setup Guide for Ollama {#Setup-Guide-for-Ollama}&quot;">​</a></h2><p>Ollama runs a background service hosting LLMs that you can access via a simple API. It&#39;s especially useful when you&#39;re working with some sensitive data that should not be sent anywhere.</p><p>Installation is very easy, just download the latest version <a href="https://ollama.ai/download" target="_blank" rel="noreferrer">here</a>.</p><p>Once you&#39;ve installed it, just launch the app and you&#39;re ready to go!</p><p>To check if it&#39;s running, go to your browser and open <code>127.0.0.1:11434</code>. You should see the message &quot;Ollama is running&quot;. Alternatively, you can run <code>ollama serve</code> in your terminal and you&#39;ll get a message that it&#39;s already running.</p><p>There are many models available in <a href="https://ollama.ai/library" target="_blank" rel="noreferrer">Ollama Library</a>, including Llama2, CodeLlama, SQLCoder, or my personal favorite <code>openhermes2.5-mistral</code>.</p><p>Download new models with <code>ollama pull &lt;model_name&gt;</code> (eg, <code>ollama pull openhermes2.5-mistral</code>).</p><p>Show currently available models with <code>ollama list</code>.</p><p>See <a href="https://ollama.ai/" target="_blank" rel="noreferrer">Ollama.ai</a> for more information.</p><h2 id="Changing-the-Default-Model-or-Schema" tabindex="-1">Changing the Default Model or Schema <a class="header-anchor" href="#Changing-the-Default-Model-or-Schema" aria-label="Permalink to &quot;Changing the Default Model or Schema {#Changing-the-Default-Model-or-Schema}&quot;">​</a></h2><p>If you tend to use non-default options, it can get tedious to specify <code>PT.*</code> every time.</p><p>There are three ways how you can customize your workflows (especially when you use Ollama or other local models):</p><ol><li><p>Import the functions/types you need explicitly at the top (eg, <code>using PromptingTools: OllamaSchema</code>)</p></li><li><p>Register your model and its associated schema (<code>PT.register_model!(; name=&quot;123&quot;, schema=PT.OllamaSchema())</code>). You won&#39;t have to specify the schema anymore only the model name. See <a href="/PromptingTools.jl/dev/frequently_asked_questions#working-with-ollama">Working with Ollama</a> for more information.</p></li><li><p>Override your default model (<code>PT.MODEL_CHAT</code>) and schema (<code>PT.PROMPT_SCHEMA</code>). It can be done persistently with Preferences, eg, <code>PT.set_preferences!(&quot;PROMPT_SCHEMA&quot; =&gt; &quot;OllamaSchema&quot;, &quot;MODEL_CHAT&quot;=&gt;&quot;llama2&quot;)</code>.</p></li></ol><h2 id="Using-Custom-API-Providers-like-Azure-or-Databricks" tabindex="-1">Using Custom API Providers like Azure or Databricks <a class="header-anchor" href="#Using-Custom-API-Providers-like-Azure-or-Databricks" aria-label="Permalink to &quot;Using Custom API Providers like Azure or Databricks {#Using-Custom-API-Providers-like-Azure-or-Databricks}&quot;">​</a></h2><p>Several providers are directly supported (eg, Databricks), check the available prompt schemas (eg, <code>subtypes(PT.AbstractOpenAISchema)</code>).</p><p>If you need a custom URL or a few keyword parameters, refer to the implementation of DatabricksOpenAISchema. You effectively need to create your own prompt schema (<code>struct MySchema &lt;: PT.AbstractOpenAISchema</code>) and override the OpenAI.jl behavior. The easiest way is to provide your custom method for <code>OpenAI.create_chat</code> and customize the <code>url</code>, <code>api_key</code>, and other <code>kwargs</code> fields. You can follow the implementation of <code>create_chat</code> for <code>DatabricksOpenAISchema</code> in <code>src/llm_openAI.jl</code>.</p><p>Once your schema is ready, you can register the necessary models via <code>PT.register_model!(; name=&quot;myschema&quot;, schema=MySchema())</code>. You can also add aliases for easier access (eg, <code>PT.MODEL_ALIASES[&quot;mymodel&quot;] = &quot;my-model-with-really-long-name&quot;</code>).</p><p>If you would like to use some heavily customized API, eg, your company&#39;s internal LLM proxy (to change headers, URL paths, etc.), refer to the example <code>examples/adding_custom_API.jl</code> in the repo.</p><h2 id="How-to-have-Multi-turn-Conversations?" tabindex="-1">How to have Multi-turn Conversations? <a class="header-anchor" href="#How-to-have-Multi-turn-Conversations?" aria-label="Permalink to &quot;How to have Multi-turn Conversations? {#How-to-have-Multi-turn-Conversations?}&quot;">​</a></h2><p>Let&#39;s say you would like to respond back to a model&#39;s response. How to do it?</p><ol><li>With <code>ai&quot;&quot;</code> macro</li></ol><p>The simplest way if you used <code>ai&quot;&quot;</code> macro, is to send a reply with the <code>ai!&quot;&quot;</code> macro. It will use the last response as the conversation.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">ai</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hi! I&#39;m John&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">ai!</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What&#39;s my name?&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Return: &quot;Your name is John.&quot;</span></span></code></pre></div><ol><li>With <code>aigenerate</code> function</li></ol><p>You can use the <code>conversation</code> keyword argument to pass the previous conversation (in all <code>ai*</code> functions). It will prepend the past <code>conversation</code> before sending the new request to the model.</p><p>To get the conversation, set <code>return_all=true</code> and store the whole conversation thread (not just the last message) in a variable. Then, use it as a keyword argument in the next call.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">conversation </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hi! I&#39;m John&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; return_all</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@info</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> last</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(conversation) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># display the response</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># follow-up (notice that we provide past messages as conversation kwarg</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">conversation </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What&#39;s my name?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; return_all</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, conversation)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## [ Info: Tokens: 50 @ Cost: $0.0 in 1.0 seconds</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## 5-element Vector{PromptingTools.AbstractMessage}:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##  PromptingTools.SystemMessage(&quot;Act as a helpful AI assistant&quot;)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##  PromptingTools.UserMessage(&quot;Hi! I&#39;m John&quot;)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##  AIMessage(&quot;Hello John! How can I assist you today?&quot;)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##  PromptingTools.UserMessage(&quot;What&#39;s my name?&quot;)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##  AIMessage(&quot;Your name is John.&quot;)</span></span></code></pre></div><p>Notice that the last message is the response to the second request, but with <code>return_all=true</code> we can see the whole conversation from the beginning.</p><h2 id="How-to-have-typed-responses?" tabindex="-1">How to have typed responses? <a class="header-anchor" href="#How-to-have-typed-responses?" aria-label="Permalink to &quot;How to have typed responses? {#How-to-have-typed-responses?}&quot;">​</a></h2><p>Our responses are always in <code>AbstractMessage</code> types to ensure we can also handle downstream processing, error handling, and self-healing code (see <code>airetry!</code>).</p><p>A good use case for a typed response is when you have a complicated control flow and would like to group and handle certain outcomes differently. You can easily do it as an extra step after the response is received.</p><p>Trivially, we can use <code>aiclassifier</code> for Bool statements, eg,</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># We can do either</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mybool </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tryparse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Bool, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aiclassify</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Is two plus two four?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">isa</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Bool </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># true</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># or simply check equality</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aiclassify</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Is two plus two four?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># true</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mybool </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> msg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;true&quot;</span></span></code></pre></div><p>Now a more complicated example with multiple categories mapping to an enum:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">choices </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;A&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;any animal or creature&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;P&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;for any plant or tree&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;O&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;for everything else&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Set up the return types we want</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">@enum</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Categories A P O</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">string_to_category </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Dict</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;A&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> A, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;P&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> P,</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;O&quot;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> O)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Run an example</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">input </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;spider&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aiclassify</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:InputClassifier</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; choices, input)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mytype </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> string_to_category[msg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A (for animal)</span></span></code></pre></div><p>How does it work? <code>aiclassify</code> guarantees to output one of our choices (and it handles some of the common quirks)!</p><p>How would we achieve the same with <code>aigenerate</code> and arbitrary struct? We need to use the &quot;lazy&quot; <code>AIGenerate</code> struct and <code>airetry!</code> to ensure we get the response and then we can process it further.</p><p><code>AIGenerate</code> has two fields you should know about:</p><ul><li><p><code>conversation</code> - eg, the vector of &quot;messages&quot; in the current conversation (same as what you get from <code>aigenerate</code> with <code>return_all=true</code>)</p></li><li><p><code>success</code> - a boolean flag if the request was successful AND if it passed any subsequent <code>airetry!</code> calls</p></li></ul><p>Let&#39;s mimic a case where our &quot;program&quot; should return one of three types: <code>SmallInt</code>, <code>LargeInt</code>, <code>FailedResponse</code>.</p><p>We first need to define our custom types:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># not needed, just to show a fully typed example</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">abstract type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MyAbstractResponse </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SmallInt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> MyAbstractResponse</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    number</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LargeInt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> MyAbstractResponse</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    number</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Int</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">struct</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> FailedResponse </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> MyAbstractResponse</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    content</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">String</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><p>Let&#39;s define our &quot;program&quot; as a function to be cleaner. Notice that we use <code>AIGenerate</code> and <code>airetry!</code> to ensure we get the response and then we can process it further.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Experimental</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">AgentTools</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> give_me_number</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">String</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">::</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">MyAbstractResponse</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # Generate the response</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> AIGenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prompt; config</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">RetryConfig</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(;max_retries</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> run!</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # Check if it&#39;s parseable as Int, if not, send back to be fixed</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # syntax: airetry!(CONDITION-TO-CHECK, &lt;response object&gt;, FEEDBACK-TO-MODEL)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    airetry!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tryparse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Int,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">last_output</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x))</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">isnothing, response, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Wrong output format! Answer with digits and nothing else. The number is:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">success </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">!=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        ## we failed to generate a parseable integer</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> FailedResponse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;I failed to get the response. Last output: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">$(last_output(response))</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    number </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tryparse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Int,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">last_output</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> number </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1000</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> ?</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> SmallInt</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(number) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> LargeInt</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(number)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">give_me_number</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How many car seats are in Porsche 911T?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## [ Info: Condition not met. Retrying...</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## [ Info: Condition not met. Retrying...</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## SmallInt(2)</span></span></code></pre></div><p>We ultimately received our custom type <code>SmallInt</code> with the number of car seats in the Porsche 911T (I hope it&#39;s correct!).</p><p>If you want to access the full conversation history (all the attempts and feedback), simply output the <code>response</code> object and explore <code>response.conversation</code>.</p><h2 id="How-to-quickly-create-a-prompt-template?" tabindex="-1">How to quickly create a prompt template? <a class="header-anchor" href="#How-to-quickly-create-a-prompt-template?" aria-label="Permalink to &quot;How to quickly create a prompt template? {#How-to-quickly-create-a-prompt-template?}&quot;">​</a></h2><p>Many times, you will want to create a prompt template that you can reuse with different inputs (eg, to create templates for AIHelpMe or LLMTextAnalysis).</p><p>Previously, you would have to create a vector of <code>SystemMessage</code> and <code>UserMessage</code> objects and then save it to a disk and reload. Now, you can use the <code>create_template</code> function to do it for you. It&#39;s designed for quick prototyping, so it skips the serialization step and loads it directly into the template store (ie, you can use it like any other templates - try <code>aitemplates()</code> search).</p><p>The syntax is simple: <code>create_template(;user=&lt;user prompt&gt;, system=&lt;system prompt&gt;, load_as=&lt;template name&gt;)</code></p><p>When called it creates a vector of messages, which you can use directly in the <code>ai*</code> functions. If you provide <code>load_as</code>, it will load the template in the template store (under the <code>load_as</code> name).</p><p>Let&#39;s generate a quick template for a simple conversation (only one placeholder: name)</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># first system message, then user message (or use kwargs)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tpl</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">create_template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You must speak like a pirate&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Say hi to {{name}}&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; load_as</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;GreatingPirate&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## 2-element Vector{PromptingTools.AbstractChatMessage}:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## PromptingTools.SystemMessage(&quot;You must speak like a pirate&quot;)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##  PromptingTools.UserMessage(&quot;Say hi to {{name}}&quot;)</span></span></code></pre></div><p>You can immediately use this template in <code>ai*</code> functions:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(tpl; name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Jack Sparrow&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Output: AIMessage(&quot;Arr, me hearty! Best be sending me regards to Captain Jack Sparrow on the salty seas! May his compass always point true to the nearest treasure trove. Yarrr!&quot;)</span></span></code></pre></div><p>Since we provided <code>load_as</code>, it&#39;s also registered in the template store:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aitemplates</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;pirate&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## 1-element Vector{AITemplateMetadata}:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">## PromptingTools.AITemplateMetadata</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   name: Symbol GreatingPirate</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   description: String &quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   version: String &quot;1.0&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   wordcount: Int64 46</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   variables: Array{Symbol}((1,))</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   system_preview: String &quot;You must speak like a pirate&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   user_preview: String &quot;Say hi to {{name}}&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">##   source: String &quot;&quot;</span></span></code></pre></div><p>So you can use it like any other template:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:GreatingPirate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Jack Sparrow&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Output: AIMessage(&quot;Arr, me hearty! Best be sending me regards to Captain Jack Sparrow on the salty seas! May his compass always point true to the nearest treasure trove. Yarrr!&quot;)</span></span></code></pre></div><p>If you want to save it in your project folder:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">save_template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;templates/GreatingPirate.json&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tpl; version</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1.0&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># optionally, add description</span></span></code></pre></div><p>It will be saved and accessed under its basename, ie, <code>GreatingPirate</code> (same as <code>load_as</code> keyword argument).</p><p>Note: If you make any changes to the templates on the disk/in a folder, you need to explicitly reload all templates again!</p><p>If you are using the main PromptingTools templates, you can simply call <code>PT.load_templates!()</code>. If you have a project folder with your templates, you want to add it first:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">load_templates!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;templates&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>After the first run, we will remember the folder and you can simply call <code>PT.load_templates!()</code> to reload all the templates in the future!</p><h2 id="What-happened-to-RAGTools?-Where-did-the-RAG-functionality-go?" tabindex="-1">What happened to RAGTools? Where did the RAG functionality go? <a class="header-anchor" href="#What-happened-to-RAGTools?-Where-did-the-RAG-functionality-go?" aria-label="Permalink to &quot;What happened to RAGTools? Where did the RAG functionality go? {#What-happened-to-RAGTools?-Where-did-the-RAG-functionality-go?}&quot;">​</a></h2><p>RAG (Retrieval-Augmented Generation) functionality has been moved to a dedicated package called <a href="https://github.com/JuliaGenAI/RAGTools.jl" target="_blank" rel="noreferrer">RAGTools.jl</a> since PromptingTools v0.75.</p><p><strong>Quick Migration:</strong></p><ol><li><p>Install RAGTools: <code>using Pkg; Pkg.add(&quot;RAGTools&quot;)</code></p></li><li><p>Change imports: <code>using RAGTools</code> (instead of <code>using PromptingTools.Experimental.RAGTools</code>)</p></li><li><p>Everything else stays the same!</p></li></ol><p><strong>Why the change?</strong></p><ul><li><p><strong>Focused Development</strong>: RAGTools can evolve independently with RAG-specific features</p></li><li><p><strong>Lighter Dependencies</strong>: PromptingTools.jl is now lighter without heavy RAG dependencies</p></li><li><p><strong>Better Ecosystem</strong>: Centralized location for RAG contributions and discussions</p></li></ul><p><strong>Need Help?</strong></p><ul><li><p>📖 <a href="/PromptingTools.jl/dev/ragtools_migration#Migrating-from-PromptingTools.Experimental.RAGTools-to-RAGTools.jl">Migrating from PromptingTools.Experimental.RAGTools to RAGTools.jl</a> - Step-by-step instructions with examples</p></li><li><p>🔧 <a href="/PromptingTools.jl/dev/extra_tools/rag_tools_intro#RAG-Tools-Introduction">RAG Tools Introduction</a> - Overview of capabilities and examples</p></li><li><p>💬 <a href="https://julialang.slack.com/archives/C06G90C697X" target="_blank" rel="noreferrer">Julia Slack #generative-ai</a> - Community support</p></li></ul><p>The API is 100% compatible - you only need to change your imports!</p><h2 id="Do-we-have-a-RecursiveCharacterTextSplitter-like-Langchain?" tabindex="-1">Do we have a RecursiveCharacterTextSplitter like Langchain? <a class="header-anchor" href="#Do-we-have-a-RecursiveCharacterTextSplitter-like-Langchain?" aria-label="Permalink to &quot;Do we have a RecursiveCharacterTextSplitter like Langchain? {#Do-we-have-a-RecursiveCharacterTextSplitter-like-Langchain?}&quot;">​</a></h2><p>Yes, we do! Look for utility <code>recursive_spliter</code> (previously known as <code>split_by_length</code>). See its docstring for more information.</p><p>For reference, Langchain&#39;s <a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter" target="_blank" rel="noreferrer"><code>RecursiveCharacterTextSplitter</code></a> uses the following setting: <code>separators = [&quot;\n\n&quot;, &quot;\n&quot;, &quot; &quot;, &quot;&quot;]</code>.</p><p>I&#39;d recommend using the following instead: <code>separators = [&quot;\\n\\n&quot;, &quot;. &quot;, &quot;\\n&quot;, &quot; &quot;]</code> (ie, it does not split words, which tends to be unnecessary and quite damaging to the chunk quality).</p><p>Example:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> recursive_splitter</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">text </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Paragraph 1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\n\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Paragraph 2. Sentence 1. Sentence 2.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Paragraph 3&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">separators </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\n\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;. &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot; &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># split by paragraphs, sentences, and newlines, and words</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">chunks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> recursive_splitter</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(text, separators, max_length</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h2 id="How-would-I-fine-tune-a-model?" tabindex="-1">How would I fine-tune a model? <a class="header-anchor" href="#How-would-I-fine-tune-a-model?" aria-label="Permalink to &quot;How would I fine-tune a model? {#How-would-I-fine-tune-a-model?}&quot;">​</a></h2><p>Fine-tuning is a powerful technique to adapt a model to your specific use case (mostly the format/syntax/task). It requires a dataset of examples, which you can now easily generate with PromptingTools.jl!</p><ol><li><p>You can save any conversation (vector of messages) to a file with <code>PT.save_conversation(&quot;filename.json&quot;, conversation)</code>.</p></li><li><p>Once the finetuning time comes, create a bundle of ShareGPT-formatted conversations (common finetuning format) in a single <code>.jsonl</code> file. Use <code>PT.save_conversations(&quot;dataset.jsonl&quot;, [conversation1, conversation2, ...])</code> (notice that plural &quot;conversationS&quot; in the function name).</p></li></ol><p>For an example of an end-to-end finetuning process, check out our sister project <a href="https://github.com/svilupp/Julia-LLM-Leaderboard/blob/main/experiments/cheater-7b-finetune/README.md" target="_blank" rel="noreferrer">JuliaLLMLeaderboard Finetuning experiment</a>. It shows the process of finetuning for half a dollar with <a href="https://jarvislabs.ai/templates/axolotl" target="_blank" rel="noreferrer">JarvisLabs.ai</a> and <a href="https://github.com/OpenAccess-AI-Collective/axolotl" target="_blank" rel="noreferrer">Axolotl</a>.</p><h2 id="Can-I-see-how-my-prompt-is-rendered-/-what-is-sent-to-the-API?" tabindex="-1">Can I see how my prompt is rendered / what is sent to the API? <a class="header-anchor" href="#Can-I-see-how-my-prompt-is-rendered-/-what-is-sent-to-the-API?" aria-label="Permalink to &quot;Can I see how my prompt is rendered / what is sent to the API? {#Can-I-see-how-my-prompt-is-rendered-/-what-is-sent-to-the-API?}&quot;">​</a></h2><p>Yes, there are two ways.</p><ol><li><p>&quot;dry run&quot;, where the <code>ai*</code> function will return the prompt rendered in the style of the selected API provider</p></li><li><p>&quot;partial render&quot;, for provider-agnostic purposes, you can run only the first step of the rendering pipeline to see the messages that will be sent (but formatted as <code>SystemMessage</code> and <code>UserMessage</code>), which is easy to read and work with</p></li><li><p>Dry Run</p></li></ol><p>Add kwargs <code>dry_run</code> and <code>return_all</code> to see what could have been sent to the API to your <code>ai*</code> functions (without <code>return_all</code> there is nothing to show you).</p><p>Example for OpenAI:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">dry_conv </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:BlankSystemUser</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; system </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;I exist&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, user </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;say hi&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;lngpt3t&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, return_all </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, dry_run </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>2-element Vector{Dict{String, Any}}:</span></span>
<span class="line"><span> Dict(&quot;role&quot; =&gt; &quot;system&quot;, &quot;content&quot; =&gt; &quot;I exist&quot;)</span></span>
<span class="line"><span> Dict(&quot;role&quot; =&gt; &quot;user&quot;, &quot;content&quot; =&gt; &quot;say hi&quot;)</span></span></code></pre></div><ol><li>Partial Render</li></ol><p>Personally, I prefer to see the pretty formatting of PromptingTools *Messages. To see what will be sent to the model, you can <code>render</code> only the first stage of the rendering pipeline with schema <code>NoSchema()</code> (it merely does the variable replacements and creates the necessary messages). It&#39;s shared by all the schema/providers.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">render</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">NoSchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(), </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;say hi, {{name}}&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;John&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>2-element Vector{PromptingTools.AbstractMessage}:</span></span>
<span class="line"><span> PromptingTools.SystemMessage(&quot;Act as a helpful AI assistant&quot;)</span></span>
<span class="line"><span> PromptingTools.UserMessage(&quot;say hi, John&quot;)</span></span></code></pre></div><p>What about the prompt templates? Prompt templates have an extra pre-rendering step that expands the symbolic <code>:name</code> (understood by PromptingTools as a reference to <code>AITemplate(:name)</code>) into a vector of Messages.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># expand the template into messages</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tpl </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">render</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">AITemplate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:BlankSystemUser</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">render</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">NoSchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(), tpl; system </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;I exist&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, user </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;say hi&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># replace any variables, etc.</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>2-element Vector{PromptingTools.AbstractMessage}:</span></span>
<span class="line"><span> PromptingTools.SystemMessage(&quot;I exist&quot;)</span></span>
<span class="line"><span> PromptingTools.UserMessage(&quot;say hi&quot;)</span></span></code></pre></div><p>For more information about the rendering pipeline and examples refer to <a href="/PromptingTools.jl/dev/how_it_works#Walkthrough-Example-for-aigenerate">Walkthrough Example for aigenerate</a>.</p><h2 id="Automatic-Logging-/-Tracing" tabindex="-1">Automatic Logging / Tracing <a class="header-anchor" href="#Automatic-Logging-/-Tracing" aria-label="Permalink to &quot;Automatic Logging / Tracing {#Automatic-Logging-/-Tracing}&quot;">​</a></h2><p>If you would like to automatically capture metadata about your conversations, you can use the <code>TracerSchema</code>. It automatically captures the necessary metadata such as model, task (<code>parent_id</code>), current thread (<code>thread_id</code>), API kwargs used and any prompt templates (and its versions).</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> TracerSchema, OpenAISchema</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">wrap_schema </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> TracerSchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OpenAISchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">())</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(wrap_schema, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Say hi!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># output type should be TracerMessage</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">isa</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> TracerMessage</span></span></code></pre></div><p>You can work with the message like any other message (properties of the inner <code>object</code> are overloaded). You can extract the original message with <code>unwrap</code>:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">unwrap</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(msg) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">isa</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> String</span></span></code></pre></div><p>You can extract the metadata with <code>meta</code>:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">meta</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(msg) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">isa</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Dict</span></span></code></pre></div><p>If you would like to automatically save the conversations, you can use the <code>SaverSchema</code>. It automatically serializes the conversation to a file in the directory specified by the environment variable <code>LOG_DIR</code>.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SaverSchema</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">wrap_schema </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> SaverSchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OpenAISchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">())</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">msg </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(wrap_schema, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Say hi!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>See <code>LOG_DIR</code> location to find the serialized conversation.</p><p>You can also compose multiple tracing schemas. For example, you can capture metadata with <code>TracerSchema</code> and then save everything automatically with <code>SaverSchema</code>:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> TracerSchema, SaverSchema, OpenAISchema</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">wrap_schema </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> OpenAISchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> TracerSchema </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SaverSchema</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">conv </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aigenerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(wrap_schema,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:BlankSystemUser</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; system</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You&#39;re a French-speaking assistant!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    user</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Say hi!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, api_kwargs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(;temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), return_all</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p><code>conv</code> is a vector of tracing messages that will be saved to a JSON together with metadata about the template and <code>api_kwargs</code>.</p><p>If you would like to enable this behavior automatically, you can register your favorite model (or re-register existing models) with the &quot;wrapped&quot; schema:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PT</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">register_model!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;gpt-3.5-turbo&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, schema</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OpenAISchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> TracerSchema </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">|&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SaverSchema)</span></span></code></pre></div></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/svilupp/PromptingTools.jl/edit/main/docs/src/frequently_asked_questions.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/PromptingTools.jl/dev/extra_tools/api_tools_intro" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>APITools</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/PromptingTools.jl/dev/prompts/general" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>General</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://documenter.juliadocs.org/stable/" target="_blank"><strong>Documenter.jl</strong></a> & <a href="https://vitepress.dev" target="_blank"><strong>VitePress</strong></a> & Icons by <a target="_blank" href="https://icons8.com">Icons8</a> <br></p><p class="copyright" data-v-c970a860>© Copyright 2025.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"coverage_of_model_providers.md\":\"C7SrPq5H\",\"examples_readme_examples.md\":\"B1OelIKu\",\"examples_working_with_aitemplates.md\":\"DOUZ3wO1\",\"examples_working_with_custom_apis.md\":\"D_cAm3ol\",\"examples_working_with_google_ai_studio.md\":\"wtWUNzSI\",\"examples_working_with_ollama.md\":\"CNXyK66g\",\"extra_tools_agent_tools_intro.md\":\"Bv-FsX6m\",\"extra_tools_api_tools_intro.md\":\"DUMdUROu\",\"extra_tools_rag_tools_intro.md\":\"DuVOw0wA\",\"extra_tools_text_utilities_intro.md\":\"CwKfkzNJ\",\"frequently_asked_questions.md\":\"DAORlXGh\",\"getting_started.md\":\"CPWvNKjw\",\"how_it_works.md\":\"CFtmWRCE\",\"index.md\":\"DO_7hYgz\",\"prompts_agents.md\":\"-8Is0y2x\",\"prompts_classification.md\":\"BGX3S7Li\",\"prompts_critic.md\":\"BNrzv4Cv\",\"prompts_extraction.md\":\"CGkatK0K\",\"prompts_general.md\":\"DvDITREP\",\"prompts_persona-task.md\":\"CQIIXzJb\",\"prompts_rag.md\":\"VI_UVlJ8\",\"prompts_visual.md\":\"j2z9f3KS\",\"ragtools_migration.md\":\"Bwrk8rCL\",\"reference.md\":\"CNcwOnpp\",\"reference_agenttools.md\":\"CxQewLkm\",\"reference_apitools.md\":\"NADcWXHW\",\"reference_experimental.md\":\"9X99Khg7\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"PromptingTools.jl\",\"description\":\"Streamline Your Interactions with GenAI Models. Discover the power of GenerativeAI and build mini workflows to save you 20 minutes every day.\",\"base\":\"/PromptingTools.jl/dev/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"src\":\"/logo.png\",\"width\":24,\"height\":24},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Manual\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/getting_started\"},{\"text\":\"How It Works\",\"link\":\"/how_it_works\"},{\"text\":\"Coverage of Model Providers\",\"link\":\"/coverage_of_model_providers\"},{\"text\":\"Examples\",\"items\":[{\"text\":\"Various examples\",\"link\":\"/examples/readme_examples\"},{\"text\":\"Using AITemplates\",\"link\":\"/examples/working_with_aitemplates\"},{\"text\":\"Local models with Ollama.ai\",\"link\":\"/examples/working_with_ollama\"},{\"text\":\"Google AIStudio\",\"link\":\"/examples/working_with_google_ai_studio\"},{\"text\":\"Custom APIs (Mistral, Llama.cpp)\",\"link\":\"/examples/working_with_custom_apis\"}]},{\"text\":\"Extra Tools\",\"items\":[{\"text\":\"Text Utilities\",\"link\":\"/extra_tools/text_utilities_intro\"},{\"text\":\"AgentTools\",\"link\":\"/extra_tools/agent_tools_intro\"},{\"text\":\"RAGTools\",\"link\":\"/extra_tools/rag_tools_intro\"},{\"text\":\"RAGTools Migration\",\"link\":\"/ragtools_migration\"},{\"text\":\"APITools\",\"link\":\"/extra_tools/api_tools_intro\"}]}]},{\"text\":\"F.A.Q.\",\"link\":\"/frequently_asked_questions\"},{\"text\":\"Prompt Templates\",\"items\":[{\"text\":\"General\",\"link\":\"/prompts/general\"},{\"text\":\"Persona-Task\",\"link\":\"/prompts/persona-task\"},{\"text\":\"Visual\",\"link\":\"/prompts/visual\"},{\"text\":\"Classification\",\"link\":\"/prompts/classification\"},{\"text\":\"Extraction\",\"link\":\"/prompts/extraction\"},{\"text\":\"Agents\",\"link\":\"/prompts/agents\"},{\"text\":\"RAG\",\"link\":\"/prompts/RAG\"}]},{\"text\":\"Reference\",\"items\":[{\"text\":\"PromptingTools.jl\",\"link\":\"/reference\"},{\"text\":\"Experimental Modules\",\"link\":\"/reference_experimental\"},{\"text\":\"AgentTools\",\"link\":\"/reference_agenttools\"},{\"text\":\"APITools\",\"link\":\"/reference_apitools\"}]}],\"sidebar\":[{\"text\":\"Home\",\"link\":\"/index\"},{\"text\":\"Manual\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/getting_started\"},{\"text\":\"How It Works\",\"link\":\"/how_it_works\"},{\"text\":\"Coverage of Model Providers\",\"link\":\"/coverage_of_model_providers\"},{\"text\":\"Examples\",\"collapsed\":true,\"items\":[{\"text\":\"Various examples\",\"link\":\"/examples/readme_examples\"},{\"text\":\"Using AITemplates\",\"link\":\"/examples/working_with_aitemplates\"},{\"text\":\"Local models with Ollama.ai\",\"link\":\"/examples/working_with_ollama\"},{\"text\":\"Google AIStudio\",\"link\":\"/examples/working_with_google_ai_studio\"},{\"text\":\"Custom APIs (Mistral, Llama.cpp)\",\"link\":\"/examples/working_with_custom_apis\"}]},{\"text\":\"Extra Tools\",\"collapsed\":true,\"items\":[{\"text\":\"Text Utilities\",\"link\":\"/extra_tools/text_utilities_intro\"},{\"text\":\"AgentTools\",\"link\":\"/extra_tools/agent_tools_intro\"},{\"text\":\"APITools\",\"link\":\"/extra_tools/api_tools_intro\"}]}]},{\"text\":\"F.A.Q.\",\"link\":\"/frequently_asked_questions\"},{\"text\":\"Prompt Templates\",\"collapsed\":true,\"items\":[{\"text\":\"General\",\"link\":\"/prompts/general\"},{\"text\":\"Persona-Task\",\"link\":\"/prompts/persona-task\"},{\"text\":\"Visual\",\"link\":\"/prompts/visual\"},{\"text\":\"Classification\",\"link\":\"/prompts/classification\"},{\"text\":\"Extraction\",\"link\":\"/prompts/extraction\"},{\"text\":\"Agents\",\"link\":\"/prompts/agents\"},{\"text\":\"RAG\",\"link\":\"/prompts/RAG\"}]},{\"text\":\"Reference\",\"collapsed\":true,\"items\":[{\"text\":\"PromptingTools.jl\",\"link\":\"/reference\"},{\"text\":\"Experimental Modules\",\"link\":\"/reference_experimental\"},{\"text\":\"AgentTools\",\"link\":\"/reference_agenttools\"},{\"text\":\"APITools\",\"link\":\"/reference_apitools\"}]}],\"editLink\":{\"pattern\":\"https://github.com/svilupp/PromptingTools.jl/edit/main/docs/src/:path\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/svilupp/PromptingTools.jl\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://documenter.juliadocs.org/stable/\\\" target=\\\"_blank\\\"><strong>Documenter.jl</strong></a> & <a href=\\\"https://vitepress.dev\\\" target=\\\"_blank\\\"><strong>VitePress</strong></a> & Icons by <a target=\\\"_blank\\\" href=\\\"https://icons8.com\\\">Icons8</a> <br>\",\"copyright\":\"© Copyright 2025.\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>