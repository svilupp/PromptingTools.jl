<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · PromptingTools.jl</title><meta name="title" content="Reference · PromptingTools.jl"/><meta property="og:title" content="Reference · PromptingTools.jl"/><meta property="twitter:title" content="Reference · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/reference/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/reference/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/reference/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/readme_examples/">Various examples</a></li><li><a class="tocitem" href="../examples/working_with_aitemplates/">Using AITemplates</a></li><li><a class="tocitem" href="../examples/working_with_ollama/">Local models with Ollama.ai</a></li></ul></li><li><a class="tocitem" href="../frequently_asked_questions/">F.A.Q.</a></li><li class="is-active"><a class="tocitem" href>Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/docs/src/reference.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><ul><li><a href="#PromptingTools.AITemplate"><code>PromptingTools.AITemplate</code></a></li><li><a href="#PromptingTools.AITemplateMetadata"><code>PromptingTools.AITemplateMetadata</code></a></li><li><a href="#PromptingTools.AbstractPromptSchema"><code>PromptingTools.AbstractPromptSchema</code></a></li><li><a href="#PromptingTools.ChatMLSchema"><code>PromptingTools.ChatMLSchema</code></a></li><li><a href="#PromptingTools.MaybeExtract"><code>PromptingTools.MaybeExtract</code></a></li><li><a href="#PromptingTools.OllamaManagedSchema"><code>PromptingTools.OllamaManagedSchema</code></a></li><li><a href="#PromptingTools.OpenAISchema"><code>PromptingTools.OpenAISchema</code></a></li><li><a href="#PromptingTools.TestEchoOllamaManagedSchema"><code>PromptingTools.TestEchoOllamaManagedSchema</code></a></li><li><a href="#PromptingTools.TestEchoOpenAISchema"><code>PromptingTools.TestEchoOpenAISchema</code></a></li><li><a href="#PromptingTools.UserMessageWithImages-Tuple{AbstractString}"><code>PromptingTools.UserMessageWithImages</code></a></li><li><a href="#PromptingTools.X123"><code>PromptingTools.X123</code></a></li><li><a href="#PromptingTools.aiclassify-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiclassify</code></a></li><li><a href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{&lt;:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{&lt;:AbstractString}}, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a></li><li><a href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a></li><li><a href="#PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiextract</code></a></li><li><a href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a></li><li><a href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a></li><li><a href="#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiscan</code></a></li><li><a href="#PromptingTools.aitemplates-Tuple{Regex}"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.aitemplates"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.aitemplates-Tuple{AbstractString}"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.aitemplates-Tuple{Symbol}"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.function_call_signature-Tuple{Type}"><code>PromptingTools.function_call_signature</code></a></li><li><a href="#PromptingTools.load_template-Tuple{Union{AbstractString, IO}}"><code>PromptingTools.load_template</code></a></li><li><a href="#PromptingTools.load_templates!"><code>PromptingTools.load_templates!</code></a></li><li><a href="#PromptingTools.ollama_api-Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}"><code>PromptingTools.ollama_api</code></a></li><li><a href="#PromptingTools.remove_templates!-Tuple{}"><code>PromptingTools.remove_templates!</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{AITemplate}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.replace_words-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.replace_words</code></a></li><li><a href="#PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractChatMessage}}"><code>PromptingTools.save_template</code></a></li><li><a href="#PromptingTools.split_by_length-Tuple{String}"><code>PromptingTools.split_by_length</code></a></li><li><a href="#PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@aai_str</code></a></li><li><a href="#PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@ai_str</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AITemplate" href="#PromptingTools.AITemplate"><code>PromptingTools.AITemplate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AITemplate</code></pre><p>AITemplate is a template for a conversation prompt.   This type is merely a container for the template name, which is resolved into a set of messages (=prompt) by <code>render</code>.</p><p><strong>Naming Convention</strong></p><ul><li>Template names should be in CamelCase</li><li>Follow the format <code>&lt;Persona&gt;...&lt;Variable&gt;...</code> where possible, eg, <code>JudgeIsItTrue</code>, ``<ul><li>Starting with the Persona (=System prompt), eg, <code>Judge</code> = persona is meant to <code>judge</code> some provided information</li><li>Variable to be filled in with context, eg, <code>It</code> = placeholder <code>it</code></li><li>Ending with the variable name is helpful, eg, <code>JuliaExpertTask</code> for a persona to be an expert in Julia language and <code>task</code> is the placeholder name</li></ul></li><li>Ideally, the template name should be self-explanatory, eg, <code>JudgeIsItTrue</code> = persona is meant to <code>judge</code> some provided information where it is true or false</li></ul><p><strong>Examples</strong></p><p>Save time by re-using pre-made templates, just fill in the placeholders with the keyword arguments:</p><pre><code class="language-julia hljs">msg = aigenerate(:JuliaExpertAsk; ask = &quot;How do I add packages?&quot;)</code></pre><p>The above is equivalent to a more verbose version that explicitly uses the dispatch on <code>AITemplate</code>:</p><pre><code class="language-julia hljs">msg = aigenerate(AITemplate(:JuliaExpertAsk); ask = &quot;How do I add packages?&quot;)</code></pre><p>Find available templates with <code>aitemplates</code>:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;JuliaExpertAsk&quot;)
# Will surface one specific template
# 1-element Vector{AITemplateMetadata}:
# PromptingTools.AITemplateMetadata
#   name: Symbol JuliaExpertAsk
#   description: String &quot;For asking questions about Julia language. Placeholders: `ask`&quot;
#   version: String &quot;1&quot;
#   wordcount: Int64 237
#   variables: Array{Symbol}((1,))
#   system_preview: String &quot;You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun&quot;
#   user_preview: String &quot;# Question

{{ask}}&quot;
#   source: String &quot;&quot;</code></pre><p>The above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).</p><p>Search for all Julia-related templates:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;Julia&quot;)
# 2-element Vector{AITemplateMetadata}... -&gt; more to come later!</code></pre><p>If you are on VSCode, you can leverage nice tabular display with <code>vscodedisplay</code>:</p><pre><code class="language-julia hljs">using DataFrames
tmps = aitemplates(&quot;Julia&quot;) |&gt; DataFrame |&gt; vscodedisplay</code></pre><p>I have my selected template, how do I use it? Just use the &quot;name&quot; in <code>aigenerate</code> or <code>aiclassify</code>   like you see in the first example!</p><p>You can inspect any template by &quot;rendering&quot; it (this is what the LLM will see):</p><pre><code class="language-julia hljs">julia&gt; AITemplate(:JudgeIsItTrue) |&gt; PromptingTools.render</code></pre><p>See also: <code>save_template</code>, <code>load_template</code>, <code>load_templates!</code> for more advanced use cases (and the corresponding script in <code>examples/</code> folder)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L8-L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AITemplateMetadata" href="#PromptingTools.AITemplateMetadata"><code>PromptingTools.AITemplateMetadata</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Helper for easy searching and reviewing of templates. Defined on loading of each template.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AbstractPromptSchema" href="#PromptingTools.AbstractPromptSchema"><code>PromptingTools.AbstractPromptSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Defines different prompting styles based on the model training and fine-tuning.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_interface.jl#L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ChatMLSchema" href="#PromptingTools.ChatMLSchema"><code>PromptingTools.ChatMLSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>ChatMLSchema is used by many open-source chatbots, by OpenAI models (under the hood) and by several models and inferfaces (eg, Ollama, vLLM)</p><p>You can explore it on <a href="https://tiktokenizer.vercel.app/">tiktokenizer</a></p><p>It uses the following conversation structure:</p><pre><code class="nohighlight hljs">&lt;im_start&gt;system
...&lt;im_end&gt;
&lt;|im_start|&gt;user
...&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
...&lt;|im_end|&gt;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_interface.jl#L43-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.MaybeExtract" href="#PromptingTools.MaybeExtract"><code>PromptingTools.MaybeExtract</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Extract a result from the provided data, if any, otherwise set the error and message fields.</p><p><strong>Arguments</strong></p><ul><li><code>error::Bool</code>: <code>true</code> if a result is found, <code>false</code> otherwise.</li><li><code>message::String</code>: Only present if no result is found, should be short and concise.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/extraction.jl#L174-L180">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.OllamaManagedSchema" href="#PromptingTools.OllamaManagedSchema"><code>PromptingTools.OllamaManagedSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Ollama by default manages different models and their associated prompt schemas when you pass <code>system_prompt</code> and <code>prompt</code> fields to the API.</p><p>Warning: It works only for 1 system message and 1 user message, so anything more than that has to be rejected.</p><p>If you need to pass more messagese / longer conversational history, you can use define the model-specific schema directly and pass your Ollama requests with <code>raw=true</code>,   which disables and templating and schema management by Ollama.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_interface.jl#L63-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.OpenAISchema" href="#PromptingTools.OpenAISchema"><code>PromptingTools.OpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>OpenAISchema is the default schema for OpenAI models.</p><p>It uses the following conversation template:</p><pre><code class="nohighlight hljs">[Dict(role=&quot;system&quot;,content=&quot;...&quot;),Dict(role=&quot;user&quot;,content=&quot;...&quot;),Dict(role=&quot;assistant&quot;,content=&quot;...&quot;)]</code></pre><p>It&#39;s recommended to separate sections in your prompt with markdown headers (e.g. `##Answer</p><p>`).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_interface.jl#L22-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TestEchoOllamaManagedSchema" href="#PromptingTools.TestEchoOllamaManagedSchema"><code>PromptingTools.TestEchoOllamaManagedSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Echoes the user&#39;s input back to them. Used for testing the implementation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_interface.jl#L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TestEchoOpenAISchema" href="#PromptingTools.TestEchoOpenAISchema"><code>PromptingTools.TestEchoOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Echoes the user&#39;s input back to them. Used for testing the implementation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_interface.jl#L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.UserMessageWithImages-Tuple{AbstractString}" href="#PromptingTools.UserMessageWithImages-Tuple{AbstractString}"><code>PromptingTools.UserMessageWithImages</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Construct <code>UserMessageWithImages</code> with 1 or more images. Images can be either URLs or local paths.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/messages.jl#L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.X123" href="#PromptingTools.X123"><code>PromptingTools.X123</code></a> — <span class="docstring-category">Type</span></header><section><div><p>With docstring</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/precompilation.jl#L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiclassify-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aiclassify-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiclassify</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiclassify(prompt_schema::AbstractOpenAISchema, prompt::ALLOWED_PROMPT_TYPE;
api_kwargs::NamedTuple = (logit_bias = Dict(837 =&gt; 100, 905 =&gt; 100, 9987 =&gt; 100),
    max_tokens = 1, temperature = 0),
kwargs...)</code></pre><p>Classifies the given prompt/statement as true/false/unknown.</p><p>Note: this is a very simple classifier, it is not meant to be used in production. Credit goes to <a href="https://twitter.com/AAAzzam/status/1669753721574633473">AAAzzam</a>.</p><p>It uses Logit bias trick and limits the output to 1 token to force the model to output only true/false/unknown.</p><p>Output tokens used (via <code>api_kwargs</code>):</p><ul><li>837: &#39; true&#39;</li><li>905: &#39; false&#39;</li><li>9987: &#39; unknown&#39;</li></ul><p><strong>Arguments</strong></p><ul><li><code>prompt_schema::AbstractOpenAISchema</code>: The schema for the prompt.</li><li><code>prompt</code>: The prompt/statement to classify if it&#39;s a <code>String</code>. If it&#39;s a <code>Symbol</code>, it is expanded as a template via <code>render(schema,template)</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">aiclassify(&quot;Is two plus two four?&quot;) # true
aiclassify(&quot;Is two plus three a vegetable on Mars?&quot;) # false</code></pre><p><code>aiclassify</code> returns only true/false/unknown. It&#39;s easy to get the proper <code>Bool</code> output type out with <code>tryparse</code>, eg,</p><pre><code class="language-julia hljs">tryparse(Bool, aiclassify(&quot;Is two plus two four?&quot;)) isa Bool # true</code></pre><p>Output of type <code>Nothing</code> marks that the model couldn&#39;t classify the statement as true/false.</p><p>Ideally, we would like to re-use some helpful system prompt to get more accurate responses. For this reason we have templates, eg, <code>:JudgeIsItTrue</code>. By specifying the template, we can provide our statement as the expected variable (<code>it</code> in this case) See that the model now correctly classifies the statement as &quot;unknown&quot;.</p><pre><code class="language-julia hljs">aiclassify(:JudgeIsItTrue; it = &quot;Is two plus three a vegetable on Mars?&quot;) # unknown</code></pre><p>For better results, use higher quality models like gpt4, eg, </p><pre><code class="language-julia hljs">aiclassify(:JudgeIsItTrue;
    it = &quot;If I had two apples and I got three more, I have five apples now.&quot;,
    model = &quot;gpt4&quot;) # true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_openai.jl#L270-L317">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F&lt;:Function" href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiembed(prompt_schema::AbstractOllamaManagedSchema,
        doc_or_docs::Union{AbstractString, Vector{&lt;:AbstractString}},
        postprocess::F = identity;
        verbose::Bool = true,
        api_key::String = API_KEY,
        model::String = MODEL_EMBEDDING,
        http_kwargs::NamedTuple = (retry_non_idempotent = true,
                                   retries = 5,
                                   readtimeout = 120),
        api_kwargs::NamedTuple = NamedTuple(),
        kwargs...) where {F &lt;: Function}</code></pre><p>The <code>aiembed</code> function generates embeddings for the given input using a specified model and returns a message object containing the embeddings, status, token count, and elapsed time.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema::AbstractOllamaManagedSchema</code>: The schema for the prompt.</li><li><code>doc_or_docs::Union{AbstractString, Vector{&lt;:AbstractString}}</code>: The document or list of documents to generate embeddings for. The list of documents is processed sequentially,  so users should consider implementing an async version with with <code>Threads.@spawn</code></li><li><code>postprocess::F</code>: The post-processing function to apply to each embedding. Defaults to the identity function, but could be <code>LinearAlgebra.normalize</code>.</li><li><code>verbose::Bool</code>: A flag indicating whether to print verbose information. Defaults to <code>true</code>.</li><li><code>api_key::String</code>: The API key to use for the OpenAI API. Defaults to <code>API_KEY</code>.</li><li><code>model::String</code>: The model to use for generating embeddings. Defaults to <code>MODEL_EMBEDDING</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the Ollama API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: A <code>DataMessage</code> object containing the embeddings, status, token count, and elapsed time.</li></ul><p>Note: Ollama API currently does not return the token count, so it&#39;s set to <code>(0,0)</code></p><p><strong>Example</strong></p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, &quot;Hello World&quot;; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096-element JSON3.Array{Float64...</code></pre><p>We can embed multiple strings at once and they will be <code>hcat</code> into a matrix   (ie, each column corresponds to one string)</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, [&quot;Hello World&quot;, &quot;How are you?&quot;]; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096×2 Matrix{Float64}:</code></pre><p>If you plan to calculate the cosine distance between embeddings, you can normalize them first:</p><pre><code class="language-julia hljs">const PT = PromptingTools
using LinearAlgebra
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, [&quot;embed me&quot;, &quot;and me too&quot;], LinearAlgebra.normalize; model=&quot;openhermes2.5-mistral&quot;)

# calculate cosine distance between the two normalized embeddings as a simple dot product
msg.content&#39; * msg.content[:, 1] # [1.0, 0.34]</code></pre><p>Similarly, you can use the <code>postprocess</code> argument to materialize the data from JSON3.Object by using <code>postprocess = copy</code></p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, &quot;Hello World&quot;, copy; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096-element Vector{Float64}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_ollama_managed.jl#L198-L271">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{&lt;:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{&lt;:AbstractString}}, F}} where F&lt;:Function" href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{&lt;:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{&lt;:AbstractString}}, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiembed(prompt_schema::AbstractOpenAISchema,
        doc_or_docs::Union{AbstractString, Vector{&lt;:AbstractString}},
        postprocess::F = identity;
        verbose::Bool = true,
        api_key::String = API_KEY,
        model::String = MODEL_EMBEDDING,
        http_kwargs::NamedTuple = (retry_non_idempotent = true,
                                   retries = 5,
                                   readtimeout = 120),
        api_kwargs::NamedTuple = NamedTuple(),
        kwargs...) where {F &lt;: Function}</code></pre><p>The <code>aiembed</code> function generates embeddings for the given input using a specified model and returns a message object containing the embeddings, status, token count, and elapsed time.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema::AbstractOpenAISchema</code>: The schema for the prompt.</li><li><code>doc_or_docs::Union{AbstractString, Vector{&lt;:AbstractString}}</code>: The document or list of documents to generate embeddings for.</li><li><code>postprocess::F</code>: The post-processing function to apply to each embedding. Defaults to the identity function.</li><li><code>verbose::Bool</code>: A flag indicating whether to print verbose information. Defaults to <code>true</code>.</li><li><code>api_key::String</code>: The API key to use for the OpenAI API. Defaults to <code>API_KEY</code>.</li><li><code>model::String</code>: The model to use for generating embeddings. Defaults to <code>MODEL_EMBEDDING</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to <code>(retry_non_idempotent = true, retries = 5, readtimeout = 120)</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the OpenAI API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs...</code>: Additional keyword arguments.</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: A <code>DataMessage</code> object containing the embeddings, status, token count, and elapsed time.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">msg = aiembed(&quot;Hello World&quot;)
msg.content # 1536-element JSON3.Array{Float64...</code></pre><p>We can embed multiple strings at once and they will be <code>hcat</code> into a matrix   (ie, each column corresponds to one string)</p><pre><code class="language-julia hljs">msg = aiembed([&quot;Hello World&quot;, &quot;How are you?&quot;])
msg.content # 1536×2 Matrix{Float64}:</code></pre><p>If you plan to calculate the cosine distance between embeddings, you can normalize them first:</p><pre><code class="language-julia hljs">using LinearAlgebra
msg = aiembed([&quot;embed me&quot;, &quot;and me too&quot;], LinearAlgebra.normalize)

# calculate cosine distance between the two normalized embeddings as a simple dot product
msg.content&#39; * msg.content[:, 1] # [1.0, 0.787]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_openai.jl#L173-L225">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiextract</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiextract([prompt_schema::AbstractOpenAISchema,] prompt::ALLOWED_PROMPT_TYPE; 
return_type::Type,
verbose::Bool = true,
    model::String = MODEL_CHAT,
    http_kwargs::NamedTuple = (;
        retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Extract required information (defined by a struct <strong><code>return_type</code></strong>) from the provided prompt by leveraging OpenAI function calling mode.</p><p>This is a perfect solution for extracting structured information from text (eg, extract organization names in news articles, etc.)</p><p>It&#39;s effectively a light wrapper around <code>aigenerate</code> call, which requires additional keyword argument <code>return_type</code> to be provided  and will enforce the model outputs to adhere to it.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>return_type</code>: A <strong>struct</strong> TYPE representing the the information we want to extract. Do not provide a struct instance, only the type. If the struct has a docstring, it will be provided to the model as well. It&#39;s used to enforce structured model outputs or provide more information.</li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: An <code>DataMessage</code> object representing the extracted data, including the content, status, tokens, and elapsed time.  Use <code>msg.content</code> to access the extracted data.</li></ul><p>See also: <code>function_call_signature</code>, <code>MaybeExtract</code>, <code>aigenerate</code></p><p><strong>Example</strong></p><p>Do you want to extract some specific measurements from a text like age, weight and height? You need to define the information you need as a struct (<code>return_type</code>):</p><pre><code class="nohighlight hljs">&quot;Person&#39;s age, height, and weight.&quot;
struct MyMeasurement
    age::Int # required
    height::Union{Int,Nothing} # optional
    weight::Union{Nothing,Float64} # optional
end
msg = aiextract(&quot;James is 30, weighs 80kg. He&#39;s 180cm tall.&quot;; return_type=MyMeasurement)
# [ Info: Tokens: 129 @ Cost: $0.0002 in 1.0 seconds
# PromptingTools.DataMessage(MyMeasurement)
msg.content
# MyMeasurement(30, 180, 80.0)</code></pre><p>The fields that allow <code>Nothing</code> are marked as optional in the schema:</p><pre><code class="nohighlight hljs">msg = aiextract(&quot;James is 30.&quot;; return_type=MyMeasurement)
# MyMeasurement(30, nothing, nothing)</code></pre><p>If there are multiple items you want to extract, define a wrapper struct to get a Vector of <code>MyMeasurement</code>:</p><pre><code class="nohighlight hljs">struct MyMeasurementWrapper
    measurements::Vector{MyMeasurement}
end

msg = aiextract(&quot;James is 30, weighs 80kg. He&#39;s 180cm tall. Then Jack is 19 but really tall - over 190!&quot;; return_type=ManyMeasurements)

msg.content.measurements
# 2-element Vector{MyMeasurement}:
#  MyMeasurement(30, 180, 80.0)
#  MyMeasurement(19, 190, nothing)</code></pre><p>Or if you want your extraction to fail gracefully when data isn&#39;t found, use <code>MaybeExtract{T}</code> wrapper  (this trick is inspired by the Instructor package!):</p><pre><code class="nohighlight hljs">using PromptingTools: MaybeExtract

type = MaybeExtract{MyMeasurement}
# Effectively the same as:
# struct MaybeExtract{T}
#     result::Union{T, Nothing} // The result of the extraction
#     error::Bool // true if a result is found, false otherwise
#     message::Union{Nothing, String} // Only present if no result is found, should be short and concise
# end

# If LLM extraction fails, it will return a Dict with `error` and `message` fields instead of the result!
msg = aiextract(&quot;Extract measurements from the text: I am giraffe&quot;, type)
msg.content
# MaybeExtract{MyMeasurement}(nothing, true, &quot;I&#39;m sorry, but I can only assist with human measurements.&quot;)</code></pre><p>That way, you can handle the error gracefully and get a reason why extraction failed (in <code>msg.content.message</code>).</p><p>Note that the error message refers to a giraffe not being a human,   because in our <code>MyMeasurement</code> docstring, we said that it&#39;s for people!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_openai.jl#L330-L427">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aigenerate(prompt_schema::AbstractOllamaManagedSchema, prompt::ALLOWED_PROMPT_TYPE; verbose::Bool = true,
    model::String = MODEL_CHAT,
    http_kwargs::NamedTuple = NamedTuple(), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generate an AI response based on a given prompt using the OpenAI API.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code> not <code>AbstractManagedSchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: Provided for interface consistency. Not needed for locally hosted Ollama.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the Ollama API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aiembed</code></p><p><strong>Example</strong></p><p>Simple hello world to test the API:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema() # We need to explicit if we want Ollama, OpenAISchema is the default

msg = aigenerate(schema, &quot;Say hi!&quot;; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 69 in 0.9 seconds
# AIMessage(&quot;Hello! How can I assist you today?&quot;)</code></pre><p><code>msg</code> is an <code>AIMessage</code> object. Access the generated string via <code>content</code> property:</p><pre><code class="language-julia hljs">typeof(msg) # AIMessage{SubString{String}}
propertynames(msg) # (:content, :status, :tokens, :elapsed
msg.content # &quot;Hello! How can I assist you today?&quot;</code></pre><p>Note: We need to be explicit about the schema we want to use. If we don&#39;t, it will default to <code>OpenAISchema</code> (=<code>PT.DEFAULT_SCHEMA</code>) ___ You can use string interpolation:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()
a = 1
msg=aigenerate(schema, &quot;What is `$a+$a`?&quot;; model=&quot;openhermes2.5-mistral&quot;)
msg.content # &quot;The result of `1+1` is `2`.&quot;</code></pre><p>___ You can provide the whole conversation or more intricate prompts as a <code>Vector{AbstractMessage}</code>:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]

msg = aigenerate(schema, conversation; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 111 in 2.1 seconds
# AIMessage(&quot;Strong the attachment is, it leads to suffering it may. Focus on the force within you must, ...&lt;continues&gt;&quot;)</code></pre><p>Note: Managed Ollama currently supports at most 1 User Message and 1 System Message given the API limitations. If you want more, you need to use the <code>ChatMLSchema</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_ollama_managed.jl#L104-L173">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aigenerate([prompt_schema::AbstractOpenAISchema,] prompt::ALLOWED_PROMPT_TYPE; verbose::Bool = true,
    model::String = MODEL_CHAT,
    http_kwargs::NamedTuple = (;
        retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generate an AI response based on a given prompt using the OpenAI API.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code>, <code>aiscan</code></p><p><strong>Example</strong></p><p>Simple hello world to test the API:</p><pre><code class="language-julia hljs">result = aigenerate(&quot;Say Hi!&quot;)
# [ Info: Tokens: 29 @ Cost: $0.0 in 1.0 seconds
# AIMessage(&quot;Hello! How can I assist you today?&quot;)</code></pre><p><code>result</code> is an <code>AIMessage</code> object. Access the generated string via <code>content</code> property:</p><pre><code class="language-julia hljs">typeof(result) # AIMessage{SubString{String}}
propertynames(result) # (:content, :status, :tokens, :elapsed
result.content # &quot;Hello! How can I assist you today?&quot;</code></pre><p>___ You can use string interpolation:</p><pre><code class="language-julia hljs">a = 1
msg=aigenerate(&quot;What is `$a+$a`?&quot;)
msg.content # &quot;The sum of `1+1` is `2`.&quot;</code></pre><p>___ You can provide the whole conversation or more intricate prompts as a <code>Vector{AbstractMessage}</code>:</p><pre><code class="language-julia hljs">const PT = PromptingTools

conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]
msg=aigenerate(conversation)
# AIMessage(&quot;Ah, strong feelings you have for your iPhone. A Jedi&#39;s path, this is not... &lt;continues&gt;&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_openai.jl#L68-L128">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiscan</code></a> — <span class="docstring-category">Method</span></header><section><div><p>aiscan([prompt<em>schema::AbstractOpenAISchema,] prompt::ALLOWED</em>PROMPT<em>TYPE;      image</em>url::Union{Nothing, AbstractString, Vector{&lt;:AbstractString}} = nothing,     image<em>path::Union{Nothing, AbstractString, Vector{&lt;:AbstractString}} = nothing,     image</em>detail::AbstractString = &quot;auto&quot;,     attach<em>to</em>latest::Bool = true,     verbose::Bool = true,         model::String = MODEL<em>CHAT,         http</em>kwargs::NamedTuple = (;             retry<em>non</em>idempotent = true,             retries = 5,             readtimeout = 120),          api<em>kwargs::NamedTuple = = (; max</em>tokens = 2500),         kwargs...)</p><p>Scans the provided image (<code>image_url</code> or <code>image_path</code>) with the goal provided in the <code>prompt</code>.</p><p>Can be used for many multi-modal tasks, such as: OCR (transcribe text in the image), image captioning, image classification, etc.</p><p>It&#39;s effectively a light wrapper around <code>aigenerate</code> call, which uses additional keyword arguments <code>image_url</code>, <code>image_path</code>, <code>image_detail</code> to be provided.   At least one image source (url or path) must be provided.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>image_url</code>: A string or vector of strings representing the URL(s) of the image(s) to scan.</li><li><code>image_path</code>: A string or vector of strings representing the path(s) of the image(s) to scan.</li><li><code>image_detail</code>: A string representing the level of detail to include for images. Can be <code>&quot;auto&quot;</code>, <code>&quot;high&quot;</code>, or <code>&quot;low&quot;</code>. See <a href="https://platform.openai.com/docs/guides/vision">OpenAI Vision Guide</a> for more details.</li><li><code>attach_to_latest</code>: A boolean how to handle if a conversation with multiple <code>UserMessage</code> is provided. When <code>true</code>, the images are attached to the latest <code>UserMessage</code>.</li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aigenerate</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code></p><p><strong>Notes</strong></p><ul><li>All examples below use model &quot;gpt4v&quot;, which is an alias for model ID &quot;gpt-4-vision-preview&quot;</li><li><code>max_tokens</code> in the <code>api_kwargs</code> is preset to 2500, otherwise OpenAI enforces a default of only a few hundred tokens (~300). If your output is truncated, increase this value</li></ul><p><strong>Example</strong></p><p>Describe the provided image:</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=&quot;julia.png&quot;, model=&quot;gpt4v&quot;)
# [ Info: Tokens: 1141 @ Cost: $0.0117 in 2.2 seconds
# AIMessage(&quot;The image shows a logo consisting of the word &quot;julia&quot; written in lowercase&quot;)</code></pre><p>You can provide multiple images at once as a vector and ask for &quot;low&quot; level of detail (cheaper):</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=[&quot;julia.png&quot;,&quot;python.png&quot;], image_detail=&quot;low&quot;, model=&quot;gpt4v&quot;)</code></pre><p>You can use this function as a nice and quick OCR (transcribe text in the image) with a template <code>:OCRTask</code>.  Let&#39;s transcribe some SQL code from a screenshot (no more re-typing!):</p><pre><code class="language-julia hljs"># Screenshot of some SQL code
image_url = &quot;https://www.sqlservercentral.com/wp-content/uploads/legacy/8755f69180b7ac7ee76a69ae68ec36872a116ad4/24622.png&quot;
msg = aiscan(:OCRTask; image_url, model=&quot;gpt4v&quot;, task=&quot;Transcribe the SQL code in the image.&quot;, api_kwargs=(; max_tokens=2500))

# [ Info: Tokens: 362 @ Cost: $0.0045 in 2.5 seconds
# AIMessage(&quot;```sql
# update Orders &lt;continue&gt;

# You can add syntax highlighting of the outputs via Markdown
using Markdown
msg.content |&gt; Markdown.parse</code></pre><p>Notice that we enforce <code>max_tokens = 2500</code>. That&#39;s because OpenAI seems to default to ~300 tokens, which provides incomplete outputs. Hence, we set this value to 2500 as a default. If you still get truncated outputs, increase this value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_openai.jl#L472-L553">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates" href="#PromptingTools.aitemplates"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aitemplates</code></pre><p>Find easily the most suitable templates for your use case.</p><p>You can search by:</p><ul><li><code>query::Symbol</code> which looks look only for partial matches in the template <code>name</code></li><li><code>query::AbstractString</code> which looks for partial matches in the template <code>name</code> or <code>description</code></li><li><code>query::Regex</code> which looks for matches in the template <code>name</code>, <code>description</code> or any of the message previews</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>limit::Int</code> limits the number of returned templates (Defaults to 10)</li></ul><p><strong>Examples</strong></p><p>Find available templates with <code>aitemplates</code>:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;JuliaExpertAsk&quot;)
# Will surface one specific template
# 1-element Vector{AITemplateMetadata}:
# PromptingTools.AITemplateMetadata
#   name: Symbol JuliaExpertAsk
#   description: String &quot;For asking questions about Julia language. Placeholders: `ask`&quot;
#   version: String &quot;1&quot;
#   wordcount: Int64 237
#   variables: Array{Symbol}((1,))
#   system_preview: String &quot;You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun&quot;
#   user_preview: String &quot;# Question

{{ask}}&quot;
#   source: String &quot;&quot;</code></pre><p>The above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).</p><p>Search for all Julia-related templates:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;Julia&quot;)
# 2-element Vector{AITemplateMetadata}... -&gt; more to come later!</code></pre><p>If you are on VSCode, you can leverage nice tabular display with <code>vscodedisplay</code>:</p><pre><code class="language-julia hljs">using DataFrames
tmps = aitemplates(&quot;Julia&quot;) |&gt; DataFrame |&gt; vscodedisplay</code></pre><p>I have my selected template, how do I use it? Just use the &quot;name&quot; in <code>aigenerate</code> or <code>aiclassify</code>   like you see in the first example!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L221-L269">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates-Tuple{AbstractString}" href="#PromptingTools.aitemplates-Tuple{AbstractString}"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Find the top-<code>limit</code> templates whose <code>name</code> or <code>description</code> fields partially match the <code>query_key::String</code> in <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L279">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates-Tuple{Regex}" href="#PromptingTools.aitemplates-Tuple{Regex}"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Find the top-<code>limit</code> templates where provided <code>query_key::Regex</code> matches either of <code>name</code>, <code>description</code> or previews or User or System messages in <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L289">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates-Tuple{Symbol}" href="#PromptingTools.aitemplates-Tuple{Symbol}"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Find the top-<code>limit</code> templates whose <code>name::Symbol</code> partially matches the <code>query_name::Symbol</code> in <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L270">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.function_call_signature-Tuple{Type}" href="#PromptingTools.function_call_signature-Tuple{Type}"><code>PromptingTools.function_call_signature</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">function_call_signature(datastructtype::Struct; max_description_length::Int = 100)</code></pre><p>Extract the argument names, types and docstrings from a struct to create the function call signature in JSON schema.</p><p>You must provide a Struct type (not an instance of it) with some fields.</p><p>Note: Fairly experimental, but works for combination of structs, arrays, strings and singletons.</p><p><strong>Tips</strong></p><ul><li>You can improve the quality of the extraction by writing a helpful docstring for your struct (or any nested struct). It will be provided as a description. </li></ul><p>You can even include comments/descriptions about the individual fields.</p><ul><li>All fields are assumed to be required, unless you allow null values (eg, <code>::Union{Nothing, Int}</code>). Fields with <code>Nothing</code> will be treated as optional.</li><li>Missing values are ignored (eg, <code>::Union{Missing, Int}</code> will be treated as Int). It&#39;s for broader compatibility and we cannot deserialize it as easily as <code>Nothing</code>.</li></ul><p><strong>Example</strong></p><p>Do you want to extract some specific measurements from a text like age, weight and height? You need to define the information you need as a struct (<code>return_type</code>):</p><pre><code class="nohighlight hljs">struct MyMeasurement
    age::Int
    height::Union{Int,Nothing}
    weight::Union{Nothing,Float64}
end
signature = function_call_signature(MyMeasurement)
#
# Dict{String, Any} with 3 entries:
#   &quot;name&quot;        =&gt; &quot;MyMeasurement_extractor&quot;
#   &quot;parameters&quot;  =&gt; Dict{String, Any}(&quot;properties&quot;=&gt;Dict{String, Any}(&quot;height&quot;=&gt;Dict{String, Any}(&quot;type&quot;=&gt;&quot;integer&quot;), &quot;weight&quot;=&gt;Dic…
#   &quot;description&quot; =&gt; &quot;Represents person&#39;s age, height, and weight
&quot;</code></pre><p>You can see that only the field <code>age</code> does not allow null values, hence, it&#39;s &quot;required&quot;. While <code>height</code> and <code>weight</code> are optional.</p><pre><code class="nohighlight hljs">signature[&quot;parameters&quot;][&quot;required&quot;]
# [&quot;age&quot;]</code></pre><p>If there are multiple items you want to extract, define a wrapper struct to get a Vector of <code>MyMeasurement</code>:</p><pre><code class="nohighlight hljs">struct MyMeasurementWrapper
    measurements::Vector{MyMeasurement}
end

Or if you want your extraction to fail gracefully when data isn&#39;t found, use `MaybeExtract{T}` wrapper (inspired by Instructor package!):</code></pre><p>using PromptingTools: MaybeExtract</p><p>type = MaybeExtract{MyMeasurement}</p><p><strong>Effectively the same as:</strong></p><p><strong>struct MaybeExtract{T}</strong></p><p><strong>result::Union{T, Nothing}</strong></p><p><strong>error::Bool // true if a result is found, false otherwise</strong></p><p><strong>message::Union{Nothing, String} // Only present if no result is found, should be short and concise</strong></p><p><strong>end</strong></p><p><strong>If LLM extraction fails, it will return a Dict with <code>error</code> and <code>message</code> fields instead of the result!</strong></p><p>msg = aiextract(&quot;Extract measurements from the text: I am giraffe&quot;, type)</p><p><strong></strong></p><p><strong>Dict{Symbol, Any} with 2 entries:</strong></p><p><strong>:message =&gt; &quot;Sorry, this feature is only available for humans.&quot;</strong></p><p><strong>:error   =&gt; true</strong></p><p>``` That way, you can handle the error gracefully and get a reason why extraction failed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/extraction.jl#L86-L154">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.load_template-Tuple{Union{AbstractString, IO}}" href="#PromptingTools.load_template-Tuple{Union{AbstractString, IO}}"><code>PromptingTools.load_template</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Loads messaging template from <code>io_or_file</code> and returns tuple of template messages and metadata.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.load_templates!" href="#PromptingTools.load_templates!"><code>PromptingTools.load_templates!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">load_templates!(; remove_templates::Bool=true)</code></pre><p>Loads templates from folder <code>templates/</code> in the package root and stores them in <code>TEMPLATE_STORE</code> and <code>TEMPLATE_METADATA</code>.</p><p>Note: Automatically removes any existing templates and metadata from <code>TEMPLATE_STORE</code> and <code>TEMPLATE_METADATA</code> if <code>remove_templates=true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L153-L159">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ollama_api-Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}" href="#PromptingTools.ollama_api-Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}"><code>PromptingTools.ollama_api</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ollama_api(prompt_schema::AbstractOllamaManagedSchema, prompt::AbstractString,
    system::Union{Nothing, AbstractString} = nothing,
    endpoint::String = &quot;generate&quot;;
    model::String = &quot;llama2&quot;, http_kwargs::NamedTuple = NamedTuple(),
    stream::Bool = false,
    url::String = &quot;localhost&quot;, port::Int = 11434,
    kwargs...)</code></pre><p>Simple wrapper for a call to Ollama API.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>prompt_schema</code>: Defines which prompt template should be applied.</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code></li><li><code>system</code>: An optional string representing the system message for the AI conversation. If not provided, a default message will be used.</li><li><code>endpoint</code>: The API endpoint to call, only &quot;generate&quot; and &quot;embeddings&quot; are currently supported. Defaults to &quot;generate&quot;.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>stream</code>: A boolean indicating whether to stream the response. Defaults to <code>false</code>.</li><li><code>url</code>: The URL of the Ollama API. Defaults to &quot;localhost&quot;.</li><li><code>port</code>: The port of the Ollama API. Defaults to 11434.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_ollama_managed.jl#L50-L72">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.remove_templates!-Tuple{}" href="#PromptingTools.remove_templates!-Tuple{}"><code>PromptingTools.remove_templates!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    remove_templates!()</code></pre><p>Removes all templates from <code>TEMPLATE_STORE</code> and <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L146-L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{AITemplate}" href="#PromptingTools.render-Tuple{AITemplate}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Renders provided messaging template (<code>template</code>) under the default schema (<code>PROMPT_SCHEMA</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L110">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::AbstractOllamaManagedSchema,
    messages::Vector{&lt;:AbstractMessage};
    kwargs...)</code></pre><p>Builds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</p><p>Note: Due to its &quot;managed&quot; nature, at most 2 messages can be provided (<code>system</code> and <code>prompt</code> inputs in the API).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_ollama_managed.jl#L5-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::AbstractOpenAISchema,
    messages::Vector{&lt;:AbstractMessage};
    image_detail::AbstractString = &quot;auto&quot;,
    kwargs...)</code></pre><p>Builds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</p><p><strong>Arguments</strong></p><ul><li><code>image_detail</code>: Only for <code>UserMessageWithImages</code>. It represents the level of detail to include for images. Can be <code>&quot;auto&quot;</code>, <code>&quot;high&quot;</code>, or <code>&quot;low&quot;</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/llm_openai.jl#L2-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.replace_words-Tuple{AbstractString, Vector{&lt;:AbstractString}}" href="#PromptingTools.replace_words-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.replace_words</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replace_words(text::AbstractString, words::Vector{&lt;:AbstractString}; replacement::AbstractString=&quot;ABC&quot;)</code></pre><p>Replace all occurrences of words in <code>words</code> with <code>replacement</code> in <code>text</code>. Useful to quickly remove specific names or entities from a text.</p><p><strong>Arguments</strong></p><ul><li><code>text::AbstractString</code>: The text to be processed.</li><li><code>words::Vector{&lt;:AbstractString}</code>: A vector of words to be replaced.</li><li><code>replacement::AbstractString=&quot;ABC&quot;</code>: The replacement string to be used. Defaults to &quot;ABC&quot;.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">text = &quot;Disney is a great company&quot;
replace_words(text, [&quot;Disney&quot;, &quot;Snow White&quot;, &quot;Mickey Mouse&quot;])
# Output: &quot;ABC is a great company&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/utils.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractChatMessage}}" href="#PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractChatMessage}}"><code>PromptingTools.save_template</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Saves provided messaging template (<code>messages</code>) to <code>io_or_file</code>. Automatically adds metadata based on provided keyword arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/templates.jl#L117">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.split_by_length-Tuple{String}" href="#PromptingTools.split_by_length-Tuple{String}"><code>PromptingTools.split_by_length</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">split_by_length(text::String; separator::String=&quot; &quot;, max_length::Int=35000) -&gt; Vector{String}</code></pre><p>Split a given string <code>text</code> into chunks of a specified maximum length <code>max_length</code>.  This is particularly useful for splitting larger documents or texts into smaller segments, suitable for models or systems with smaller context windows.</p><p><strong>Arguments</strong></p><ul><li><code>text::String</code>: The text to be split.</li><li><code>separator::String=&quot; &quot;</code>: The separator used to split the text into minichunks. Defaults to a space character.</li><li><code>max_length::Int=35000</code>: The maximum length of each chunk. Defaults to 35,000 characters, which should fit within 16K context window.</li></ul><p><strong>Returns</strong></p><p><code>Vector{String}</code>: A vector of strings, each representing a chunk of the original text that is smaller than or equal to <code>max_length</code>.</p><p><strong>Notes</strong></p><ul><li>The function ensures that each chunk is as close to <code>max_length</code> as possible without exceeding it.</li><li>If the <code>text</code> is empty, the function returns an empty array.</li><li>The <code>separator</code> is re-added to the text chunks after splitting, preserving the original structure of the text as closely as possible.</li></ul><p><strong>Examples</strong></p><p>Splitting text with the default separator (&quot; &quot;):</p><pre><code class="language-julia hljs">text = &quot;Hello world. How are you?&quot;
chunks = splitbysize(text; max_length=13)
length(chunks) # Output: 2</code></pre><p>Using a custom separator and custom <code>max_length</code></p><pre><code class="language-julia hljs">text = &quot;Hello,World,&quot; ^ 2900 # length 34900 chars
split_by_length(text; separator=&quot;,&quot;, max_length=10000) # for 4K context window
length(chunks[1]) # Output: 4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/utils.jl#L33-L68">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}" href="#PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@aai_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">aai&quot;user_prompt&quot;[model_alias] -&gt; AIMessage</code></pre><p>Asynchronous version of <code>@ai_str</code> macro, which will log the result once it&#39;s ready.</p><p><strong>Example</strong></p><p>Send asynchronous request to GPT-4, so we don&#39;t have to wait for the response: Very practical with slow models, so you can keep working in the meantime.</p><p>```julia m = aai&quot;Say Hi!&quot;gpt4; </p><p><strong>...with some delay...</strong></p><p><strong>[ Info: Tokens: 29 @ Cost: 0.0011 in 2.7 seconds</strong></p><p><strong>[ Info: AIMessage&gt; Hello! How can I assist you today?</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/macros.jl#L40-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}" href="#PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@ai_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">ai&quot;user_prompt&quot;[model_alias] -&gt; AIMessage</code></pre><p>The <code>ai&quot;&quot;</code> string macro generates an AI response to a given prompt by using <code>aigenerate</code> under the hood.</p><p><strong>Arguments</strong></p><ul><li><code>user_prompt</code> (String): The input prompt for the AI model.</li><li><code>model_alias</code> (optional, any): Provide model alias of the AI model (see <code>MODEL_ALIASES</code>).</li></ul><p><strong>Returns</strong></p><p><code>AIMessage</code> corresponding to the input prompt.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">result = ai&quot;Hello, how are you?&quot;
# AIMessage(&quot;Hello! I&#39;m an AI assistant, so I don&#39;t have feelings, but I&#39;m here to help you. How can I assist you today?&quot;)</code></pre><p>If you want to interpolate some variables or additional context, simply use string interpolation:</p><pre><code class="language-julia hljs">a=1
result = ai&quot;What is `$a+$a`?&quot;
# AIMessage(&quot;The sum of `1+1` is `2`.&quot;)</code></pre><p>If you want to use a different model, eg, GPT-4, you can provide its alias as a flag:</p><pre><code class="language-julia hljs">result = ai&quot;What is `1.23 * 100 + 1`?&quot;gpt4
# AIMessage(&quot;The answer is 124.&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/8dfa377d0e6c00f092bbbe07c9b4f77433b9c005/src/macros.jl#L1-L31">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../frequently_asked_questions/">« F.A.Q.</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Sunday 3 December 2023 11:13">Sunday 3 December 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
